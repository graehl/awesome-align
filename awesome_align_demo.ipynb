{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graehl/awesome-align/blob/master/awesome_align_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjc7LvIQbuMn"
      },
      "source": [
        "# AWESOME: Aligning Word Embedding Spaces of Multilingual Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ipxcuO9vDgZ"
      },
      "source": [
        "[``awesome-align``](https://github.com/neulab/awesome-align) is a tool that can extract word alignments from multilingual BERT (mBERT) and allows you to fine-tune mBERT on parallel corpora for better alignment quality (see [our paper](https://arxiv.org/abs/2101.08231) for more details).\n",
        "\n",
        "This is a simple demo of how `awesome-align` extracts word alignments from mBERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJpRK-1_wQsJ"
      },
      "source": [
        "First, install and import the following packages. (Note that the original `awesome-align` tool does not require the `transformers` package.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODwJ_gQ8bnqR",
        "outputId": "5bffeae3-f6aa-4833-9890-60e3e147c894"
      },
      "source": [
        "!pwd\n",
        "!git clone https://github.com/graehl/awesome-align.git || (cd awesome-align && git pull)\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'awesome-align' already exists and is not an empty directory.\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 7 (delta 4), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (7/7), 9.51 KiB | 1.19 MiB/s, done.\n",
            "From https://github.com/graehl/awesome-align\n",
            "   68c1ff2..7128d0a  master     -> origin/master\n",
            "Updating 68c1ff2..7128d0a\n",
            "Fast-forward\n",
            " awesome_align/modeling.py |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " awesome_align_demo.ipynb  | 2098 \u001b[32m++++++\u001b[m\u001b[31m-----------------------------------------------------------\u001b[m\n",
            " 2 files changed, 193 insertions(+), 1907 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r awesome-align/requirements.txt\n",
        "import sys\n",
        "sys.path.append('/content/awesome-align')\n",
        "sys.path.append('/content')\n",
        "\n",
        "!pip install transformers\n",
        "!pip install onnx\n",
        "!pip install skl2onnx\n",
        "import torch\n",
        "import itertools\n",
        "import onnx\n",
        "from skl2onnx.helpers import onnx_helper\n"
      ],
      "metadata": {
        "id": "3u3stwJo_7p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n"
      ],
      "metadata": {
        "id": "Jr4ySKRtWotU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRvfawCbw2i7"
      },
      "source": [
        "Load the multilingual BERT model and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aPvLqT7eiry"
      },
      "source": [
        "model_name_or_path='bert-base-multilingual-cased'\n",
        "\n",
        "import transformers\n",
        "\n",
        "from awesome_align import modeling\n",
        "from awesome_align.configuration_bert import BertConfig\n",
        "from awesome_align.modeling import BertForMaskedLM\n",
        "from awesome_align.tokenization_bert import BertTokenizer\n",
        "from awesome_align.tokenization_utils import PreTrainedTokenizer\n",
        "from awesome_align.modeling_utils import PreTrainedModel\n",
        "\n",
        "def init_model_and_tokenizer(\n",
        "    model_name_or_path,\n",
        "    config_name = None,\n",
        "    cache_dir = None,\n",
        "    tokenizer_name = None,\n",
        "):\n",
        "  config_class, model_class, tokenizer_class = BertConfig, BertForMaskedLM, BertTokenizer\n",
        "  if config_name:\n",
        "      config = config_class.from_pretrained(config_name, cache_dir=cache_dir)\n",
        "  elif model_name_or_path:\n",
        "      config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "  else:\n",
        "      config = config_class()\n",
        "\n",
        "  if tokenizer_name:\n",
        "      tokenizer = tokenizer_class.from_pretrained(tokenizer_name, cache_dir=cache_dir)\n",
        "  elif model_name_or_path:\n",
        "      tokenizer = tokenizer_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "  else:\n",
        "      raise ValueError(\n",
        "          \"You are instantiating a new {} tokenizer. This is not supported, but you can do it from another script, save it,\"\n",
        "          \"and load it from here, using --tokenizer_name\".format(tokenizer_class.__name__)\n",
        "      )\n",
        "\n",
        "  modeling.PAD_ID = tokenizer.pad_token_id\n",
        "  modeling.CLS_ID = tokenizer.cls_token_id\n",
        "  modeling.SEP_ID = tokenizer.sep_token_id\n",
        "\n",
        "  if model_name_or_path:\n",
        "      model = model_class.from_pretrained(\n",
        "          model_name_or_path,\n",
        "          from_tf=bool(\".ckpt\" in model_name_or_path),\n",
        "          config=config,\n",
        "          cache_dir=cache_dir,\n",
        "      )\n",
        "  else:\n",
        "      model = model_class(config=config)\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "USE_AWESOME_ALIGN = True\n",
        "# True causes, in export to onnx, `Boolean value of Tensor with more than one value is ambiguous`\n",
        "if USE_AWESOME_ALIGN:\n",
        "  model, tokenizer = init_model_and_tokenizer(model_name_or_path)\n",
        "else:\n",
        "  model, tokenizer = transformers.AutoModel.from_pretrained(model_name_or_path), transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RPxlavmxNmj"
      },
      "source": [
        "Input *tokenized* source and target sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfDM0w2kfHyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee6db72-ce17-4933-f559-c156c745e349"
      },
      "source": [
        "src = 'I bought a new car because I was going through a midlife crisis .'\n",
        "tgt = 'Я купил новую тачку , потому что я переживал кризис среднего возраста .'\n",
        "tgt = 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
        "srctgt = f'{src} ||| {tgt}'\n",
        "fpar = 'srctgt.txt'\n",
        "with open(fpar, 'w') as f:\n",
        "  f.write(srctgt)\n",
        "if False:\n",
        "  !rm align.txt\n",
        "  !CUDA_VISIBLE_DEVICES=0 PYTHONPATH=/content/awesome-align python /content/awesome-align/run_align.py --output_file=align.txt --model_name_or_path=\"$model_name_or_path\" --data_file=\"$fpar\" --extraction='softmax' --softmax_threshold=1e-3 --batch_size=32\n",
        "!cat align.txt"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13-14 2-1 10-8 0-0 5-4 4-2 12-9 11-12 3-3 8-6 1-0 7-5 9-7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model.eval()\n",
        "# just sets mode of model, probably doesn't need to be under no_grad\n",
        "\n",
        "class ExportHidden(torch.nn.Module):\n",
        "    def __init__(self, base_model, align_layer=8):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        # For BERT, num_hidden_layers is in config\n",
        "        self.num_layers = base_model.config.num_hidden_layers\n",
        "        self.align_layer = align_layer\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        alignkw = {}\n",
        "        if self.align_layer is not None:\n",
        "          alignkw['align_layer'] = self.align_layer\n",
        "        # Run the base model with output_hidden_states=True\n",
        "        outputs = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True,\n",
        "            **alignkw\n",
        "            )\n",
        "        hidden_states = outputs[2]\n",
        "        return hidden_states[self.align_layer] if self.align_layer is not None else hidden_states\n",
        "\n",
        "def to_onnx(model, onnx_file_path, inputs=['input_ids', 'attention_mask'], outputs=['output'], dynamic=True, batch=True, align_layer=None, opset_version=14, return_tensor_names=True):\n",
        "  captions = {0 : 'batch_size', 1: 'sequence_length'} if batch else {0 : 'sequence_length'}\n",
        "  dynamic_axes = {}\n",
        "  if dynamic:\n",
        "    for k in inputs:\n",
        "      dynamic_axes[k] = captions\n",
        "    for k in outputs:\n",
        "      dynamic_axes[k] = captions\n",
        "\n",
        "  # Create dummy input data\n",
        "  batch_size = 1\n",
        "  sequence_length = 128\n",
        "  dims = (batch_size, sequence_length) if batch else (sequence_length,)\n",
        "  inputs_ones = tuple(torch.ones(dims) if x != 'input_ids' else torch.randint(0, model.config.vocab_size, dims) for x in inputs)\n",
        "\n",
        "  hasbert = hasattr(model, 'bert')\n",
        "  print(f'hasbert={hasbert}')\n",
        "  model = model.bert if hasbert else model\n",
        "  #model = ExportHidden(model, align_layer) if align_layer is not None else model\n",
        "  # Export the model to ONNX\n",
        "  torch.onnx.export(\n",
        "      model,\n",
        "      inputs_ones, #(input_ids, attention_mask),\n",
        "      onnx_file_path,\n",
        "      export_params=True,\n",
        "      opset_version=opset_version,\n",
        "      do_constant_folding=True,\n",
        "      input_names = inputs,\n",
        "      output_names = outputs,\n",
        "      dynamic_axes=dynamic_axes,\n",
        "  )\n",
        "\n",
        "  if return_tensor_names:\n",
        "    om = onnx_helper.load_onnx_model(onnx_file_path)\n",
        "    return list(onnx_helper.enumerate_model_node_outputs(om))\n",
        "  else:\n",
        "    return f\"Model exported to {onnx_file_path}\"\n",
        "\n",
        "DO_ONNX_EXPORT=False\n",
        "if DO_ONNX_EXPORT:\n",
        "  for x in to_onnx(model, \"model.onnx\"): print(str(x))\n"
      ],
      "metadata": {
        "id": "-mUNmubaq4UQ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUa-ZqUxZ8Z"
      },
      "source": [
        "Run the model and print the resulting alignments."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "# pre-processing\n",
        "def wstok(x): return x.strip().split()\n",
        "def subwords(xs): return [tokenizer.tokenize(x) for x in xs]\n",
        "def ids(xs): return [tokenizer.convert_tokens_to_ids(x) for x in xs]\n",
        "sent_src, sent_tgt = wstok(src), wstok(tgt)\n",
        "token_src, token_tgt = subwords(sent_src), subwords(sent_tgt)\n",
        "wid_src, wid_tgt = ids(token_src), ids(token_tgt)\n",
        "#def tokenizer_max_len(tokenizer): return tokenizer.max_len_single_sentence if hasattr(tokenizer, 'max_len_single_sentence') else tokenizer.model_max_length\n",
        "maxlenkw = {}\n",
        "if hasattr(tokenizer, 'model_max_length'):\n",
        "  maxlenkw['model_max_length'] = tokenizer.model_max_length\n",
        "  maxlenkw['truncation'] = True\n",
        "else:\n",
        "  maxlenkw['max_length'] = tokenizer.max_len\n",
        "\n",
        "def ids_for_model(ids, model, tokenizer): return tokenizer.prepare_for_model(list(itertools.chain(*ids)), return_tensors='pt', **maxlenkw)['input_ids']\n",
        "print(f'wid {len(wid_src)} x {len(wid_tgt)}')\n",
        "ids_src, ids_tgt = ids_for_model(wid_src, model, tokenizer), ids_for_model(wid_tgt, model, tokenizer)\n",
        "print(f'ids {len(ids_src[0])} x {len(ids_tgt[0])}')\n",
        "print(f'{ids_src}')\n",
        "print(f'{ids_tgt}')\n",
        "sub2word_map_src = []\n",
        "for i, word_list in enumerate(token_src):\n",
        "  sub2word_map_src += [i for x in word_list]\n",
        "sub2word_map_tgt = []\n",
        "for i, word_list in enumerate(token_tgt):\n",
        "  sub2word_map_tgt += [i for x in word_list]\n",
        "\n",
        "\n",
        "# alignment\n",
        "\n",
        "def sent_without_startend(batch, sent=0): return batch[sent, 1:-1]\n",
        "if USE_AWESOME_ALIGN:\n",
        "  def hiddens(model, ids, align_layer):\n",
        "    return model.bert(ids, align_layer=align_layer, attention_mask=(ids!=0))[:, 1:-1]\n",
        "else:\n",
        "  def alignvec(batch, align_layer=8, sent=0): return sent_without_startend(batch[align_layer], sent=sent)\n",
        "  def hidden(model, ids): return model(ids.unsqueeze(0), output_hidden_states=True)[2]\n",
        "  def hiddens(model, ids, align_layer):\n",
        "    return alignvec(hidden(model, ids), align_layer)\n",
        "\n",
        "for align_layer in range(8,9):\n",
        " last_align = None\n",
        " threshold = 1e-3\n",
        " for it in range(6):\n",
        "\n",
        "  if USE_AWESOME_ALIGN:\n",
        "    # get_aligned_word handles a batch.\n",
        "    print(f'{len(ids_src)} x {len(ids_tgt)}')\n",
        "    align_words = model.get_aligned_word(ids_src, ids_tgt, (sub2word_map_src,), (sub2word_map_tgt,), 'cpu', len(ids_src), len(ids_tgt), align_layer, 'softmax', threshold, True)[0]\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      out_src = hiddens(model, ids_src, align_layer)\n",
        "      out_tgt = hiddens(model, ids_tgt, align_layer)\n",
        "      #pdb.set_trace()\n",
        "      #out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "      #out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "\n",
        "      dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
        "\n",
        "      softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
        "      softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
        "      # tryalso entmax15(dot_prod, dim=...)? also TODO: before softmax mask off cls sep pad tokens\n",
        "\n",
        "      softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
        "\n",
        "      align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
        "      align_words = set()\n",
        "      for i, j in align_subwords:\n",
        "        align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
        "  align_words = sorted(list(align_words))\n",
        "  if align_words != last_align:\n",
        "    print(f\" (layer {align_layer} > {threshold:.3g}) {len(align_words)} links {align_words} for '{src}' to '{tgt}'\")\n",
        "    for x in align_words:\n",
        "      i, j = x\n",
        "      print(f'{color.BOLD}{color.BLUE}{sent_src[i]}{color.END}==={color.BOLD}{color.RED}{sent_tgt[j]}{color.END}')\n",
        "  last_align = align_words\n",
        "  threshold = threshold * 1e-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3FlRpS-dYFv",
        "outputId": "05a1147e-cc95-41b6-b32a-17f8e5b674b2"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wid 14 x 15\n",
            "ids 17 x 19\n",
            "tensor([[  101,   146, 28870,   169, 10751, 13000, 12373,   146, 10134, 19090,\n",
            "         11222,   169, 15607, 57156, 22859,   119,   102]])\n",
            "tensor([[  101, 16680, 52302, 10333, 10119, 18257, 15249, 16348, 14645, 46481,\n",
            "         10183, 10153, 22859, 10104, 10109, 16689, 22757,   119,   102]])\n",
            "1 x 1\n",
            " (layer 8 > 0.001) 13 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            "1 x 1\n",
            "1 x 1\n",
            "1 x 1\n",
            "1 x 1\n",
            "1 x 1\n",
            " (layer 8 > 1e-08) 14 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 10), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mde\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}