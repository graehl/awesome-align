{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graehl/awesome-align/blob/master/awesome_align_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjc7LvIQbuMn"
      },
      "source": [
        "# AWESOME: Aligning Word Embedding Spaces of Multilingual Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ipxcuO9vDgZ"
      },
      "source": [
        "[``awesome-align``](https://github.com/neulab/awesome-align) is a tool that can extract word alignments from multilingual BERT (mBERT) and allows you to fine-tune mBERT on parallel corpora for better alignment quality (see [our paper](https://arxiv.org/abs/2101.08231) for more details).\n",
        "\n",
        "This is a simple demo of how `awesome-align` extracts word alignments from mBERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJpRK-1_wQsJ"
      },
      "source": [
        "First, install and import the following packages. (Note that the original `awesome-align` tool does not require the `transformers` package.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODwJ_gQ8bnqR",
        "outputId": "e93a5e52-bbcf-4388-dd74-5a831a8701e0"
      },
      "source": [
        "!pwd\n",
        "!git clone https://github.com/graehl/awesome-align.git || (cd awesome-align && git pull)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'awesome-align'...\n",
            "remote: Enumerating objects: 372, done.\u001b[K\n",
            "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 372 (delta 132), reused 96 (delta 87), pack-reused 201 (from 1)\u001b[K\n",
            "Receiving objects: 100% (372/372), 589.29 KiB | 5.51 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime\n",
        "!pip install -r awesome-align/requirements.txt\n",
        "import sys\n",
        "sys.path.append('/content/awesome-align')\n",
        "sys.path.append('/content')\n",
        "import torch\n",
        "import itertools\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3u3stwJo_7p3",
        "outputId": "97b426a3-0e76-4d2b-b7a2-d50160290896"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.21.0\n",
            "Requirement already satisfied: tokenizers>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from -r awesome-align/requirements.txt (line 1)) (0.21.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from -r awesome-align/requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r awesome-align/requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r awesome-align/requirements.txt (line 4)) (2.0.2)\n",
            "Collecting boto3 (from -r awesome-align/requirements.txt (line 5))\n",
            "  Downloading boto3-1.37.28-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from -r awesome-align/requirements.txt (line 6)) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r awesome-align/requirements.txt (line 7)) (2.32.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.5.2->-r awesome-align/requirements.txt (line 1)) (0.30.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (1.3.0)\n",
            "Collecting botocore<1.38.0,>=1.37.28 (from boto3->-r awesome-align/requirements.txt (line 5))\n",
            "  Downloading botocore-1.37.28-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->-r awesome-align/requirements.txt (line 5))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->-r awesome-align/requirements.txt (line 5))\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->-r awesome-align/requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->-r awesome-align/requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->-r awesome-align/requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->-r awesome-align/requirements.txt (line 7)) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.28->boto3->-r awesome-align/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.5.2->-r awesome-align/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.5.2->-r awesome-align/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.2.0->-r awesome-align/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.28->boto3->-r awesome-align/requirements.txt (line 5)) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.37.28-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.28-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed boto3-1.37.28 botocore-1.37.28 jmespath-1.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 s3transfer-0.11.4\n",
            "Collecting skl2onnx\n",
            "  Downloading skl2onnx-1.18.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting onnx>=1.2.1 (from skl2onnx)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.11/dist-packages (from skl2onnx) (1.6.1)\n",
            "Collecting onnxconverter-common>=1.7.0 (from skl2onnx)\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.2.1->skl2onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.2.1->skl2onnx) (5.29.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxconverter-common>=1.7.0->skl2onnx) (24.2)\n",
            "Collecting protobuf>=3.20.2 (from onnx>=1.2.1->skl2onnx)\n",
            "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (3.6.0)\n",
            "Downloading skl2onnx-1.18.0-py2.py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnx, onnxconverter-common, skl2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.17.0 onnxconverter-common-1.14.0 protobuf-3.20.2 skl2onnx-1.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "3e7d15da02b84c01ad609a39d4656e43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "def print_align(align_words, desc=''):\n",
        "    print(f\"{desc} {len(align_words)} links {align_words} for '{src}' to '{tgt}'\")\n",
        "    for x in align_words:\n",
        "      i, j = x\n",
        "      print(f'{color.BOLD}{color.BLUE}{sent_src[i]}{color.END}==={color.BOLD}{color.RED}{sent_tgt[j]}{color.END}')"
      ],
      "metadata": {
        "id": "Jr4ySKRtWotU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRvfawCbw2i7"
      },
      "source": [
        "Load the multilingual BERT model and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aPvLqT7eiry"
      },
      "source": [
        "model_name_or_path='bert-base-multilingual-cased'\n",
        "model_name = model_name_or_path.split('/')[-1]\n",
        "\n",
        "import transformers\n",
        "import awesome_align\n",
        "from awesome_align import modeling\n",
        "from awesome_align.configuration_bert import BertConfig\n",
        "from awesome_align.modeling import BertForMaskedLM\n",
        "from awesome_align.tokenization_bert import BertTokenizer\n",
        "from awesome_align.tokenization_utils import PreTrainedTokenizer\n",
        "from awesome_align.modeling_utils import PreTrainedModel\n",
        "\n",
        "def init_model_and_tokenizer(\n",
        "    model_name_or_path,\n",
        "    config_name = None,\n",
        "    cache_dir = None,\n",
        "    tokenizer_name = None,\n",
        "):\n",
        "  config_class, model_class, tokenizer_class = BertConfig, BertForMaskedLM, BertTokenizer\n",
        "  if config_name:\n",
        "      config = config_class.from_pretrained(config_name, cache_dir=cache_dir)\n",
        "  elif model_name_or_path:\n",
        "      config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "  else:\n",
        "      config = config_class()\n",
        "\n",
        "  if tokenizer_name:\n",
        "      tokenizer = tokenizer_class.from_pretrained(tokenizer_name, cache_dir=cache_dir)\n",
        "  elif model_name_or_path:\n",
        "      tokenizer = tokenizer_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "  else:\n",
        "      raise ValueError(\n",
        "          \"You are instantiating a new {} tokenizer. This is not supported, but you can do it from another script, save it,\"\n",
        "          \"and load it from here, using --tokenizer_name\".format(tokenizer_class.__name__)\n",
        "      )\n",
        "\n",
        "  # pad is actually always 0\n",
        "  modeling.PAD_ID = tokenizer.pad_token_id\n",
        "  modeling.CLS_ID = tokenizer.cls_token_id\n",
        "  modeling.SEP_ID = tokenizer.sep_token_id\n",
        "\n",
        "  if model_name_or_path:\n",
        "      model = model_class.from_pretrained(\n",
        "          model_name_or_path,\n",
        "          from_tf=bool(\".ckpt\" in model_name_or_path),\n",
        "          config=config,\n",
        "          cache_dir=cache_dir,\n",
        "      )\n",
        "  else:\n",
        "      model = model_class(config=config)\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "USE_AWESOME_ALIGN = True\n",
        "# True caused, in export to onnx, `Boolean value of Tensor with more than one value is ambiguous`, but we fixed with ExportNthLayer wrapper\n",
        "if USE_AWESOME_ALIGN:\n",
        "  model, tokenizer = init_model_and_tokenizer(model_name_or_path)\n",
        "else:\n",
        "  model, tokenizer = transformers.AutoModel.from_pretrained(model_name_or_path), transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RPxlavmxNmj"
      },
      "source": [
        "Input *tokenized* source and target sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfDM0w2kfHyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d6c8f0-8e17-4a99-97a0-635706de15a7"
      },
      "source": [
        "src = 'I bought a new car because I was going through a midlife crisis .'\n",
        "tgt = 'Я купил новую тачку , потому что я переживал кризис среднего возраста .'\n",
        "tgt = 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
        "srctgt = f'{src} ||| {tgt}'\n",
        "fpar = 'srctgt.txt'\n",
        "with open(fpar, 'w') as f:\n",
        "  f.write(srctgt)\n",
        "if True:\n",
        "  !rm align.txt\n",
        "  !CUDA_VISIBLE_DEVICES=0 PYTHONPATH=/content/awesome-align python /content/awesome-align/run_align.py --output_file=align.txt --model_name_or_path=\"$model_name_or_path\" --data_file=\"$fpar\" --extraction='softmax' --softmax_threshold=1e-3 --batch_size=32\n",
        "!cat align.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'align.txt': No such file or directory\n",
            "2025-04-05 18:55:41.254384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743879341.384602    1529 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743879341.421170    1529 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading the dataset...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Extracting: 1it [00:00,  1.62it/s]\n",
            "13-14 2-1 10-8 0-0 5-4 4-2 12-9 11-12 3-3 8-6 1-0 7-5 9-7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model.eval()\n",
        "# just sets mode of model, probably doesn't need to be under no_grad\n",
        "\n",
        "def extend_mask(attention_mask, dtype=torch.float32):\n",
        "    if attention_mask.dim() == 3:\n",
        "        extended_attention_mask = attention_mask[:, None, :, :]\n",
        "    elif attention_mask.dim() == 2:\n",
        "        extended_attention_mask = attention_mask[:, None, None, :]\n",
        "    else:\n",
        "        raise ValueError(\n",
        "             \"Wrong shape for input_ids or attention_mask\"\n",
        "        )\n",
        "    extended_attention_mask = extended_attention_mask.to(dtype=dtype)\n",
        "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "    return extended_attention_mask\n",
        "\n",
        "def guess_dtype(model):\n",
        "  if hasattr(model, 'get_parameter_dtype'):\n",
        "    return model.get_parameter_dtype()\n",
        "  elif hasattr(model, 'parameters'):\n",
        "    return next(model.parameters()).dtype\n",
        "  else:\n",
        "    return torch.float32\n",
        "\n",
        "def make_ones_mask(ids):\n",
        "  shape = ids.size()\n",
        "  device = ids.device\n",
        "  attention_mask = torch.ones(shape, device=device)\n",
        "  attention_mask[ids==0] = 0\n",
        "  return attention_mask\n",
        "\n",
        "def make_extended_mask(ids, dtype=torch.float32):\n",
        "  attention_mask = make_ones_mask(ids)\n",
        "  return extend_mask(attention_mask, dtype)\n",
        "\n",
        "class ExportNthLayer(torch.nn.Module):\n",
        "    def __init__(self, base_model, align_layer_max=8):\n",
        "        super().__init__()\n",
        "        e = base_model.bert if hasattr(base_model, 'bert') else base_model\n",
        "        self.bert = e\n",
        "        self.embeddings = e.embeddings\n",
        "        # For BERT, num_hidden_layers is in config\n",
        "        self.config = e.config\n",
        "        self.num_layers = min(e.config.num_hidden_layers, align_layer_max)\n",
        "        e = e.encoder if hasattr(e, 'encoder') else e\n",
        "        self.encoder = e\n",
        "        self.layer = e.layer[:self.num_layers]\n",
        "        print(f'{self.layer}')\n",
        "\n",
        "    def forward(self, ids, attention_mask=None, position_ids=None):\n",
        "      shape = ids.size()\n",
        "      device = ids.device\n",
        "      if attention_mask is None:\n",
        "        attention_mask = make_ones_mask(ids)\n",
        "\n",
        "      # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "      # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "      extended_attention_mask = extend_mask(attention_mask, guess_dtype(self.bert))\n",
        "      input_shape = ids.size()\n",
        "      token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "      hidden_states = self.embeddings(ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
        "\n",
        "      if self.layer is not None:\n",
        "        for i, layer in enumerate(self.layer):\n",
        "          hidden_states = layer(hidden_states, attention_mask=extended_attention_mask)\n",
        "        return hidden_states\n",
        "      else:\n",
        "        return self.bert(ids, attention_mask)\n",
        "\n",
        "def to_onnx(model, onnx_file_path, inputs=['input_ids', 'attention_mask'], outputs=['output'], dynamic=True, batch=True, align_layer=None, opset_version=14, return_tensor_names=True):\n",
        "  captions = {0 : 'batch_size', 1: 'sequence_length'} if batch else {0 : 'sequence_length'}\n",
        "  dynamic_axes = {}\n",
        "  if dynamic:\n",
        "    for k in inputs:\n",
        "      dynamic_axes[k] = captions\n",
        "    for k in outputs:\n",
        "      dynamic_axes[k] = captions\n",
        "\n",
        "  # Create dummy input data\n",
        "  batch_size = 1\n",
        "  sequence_length = 128\n",
        "  dims = (batch_size, sequence_length) if batch else (sequence_length,)\n",
        "  inputs_ones = tuple(torch.ones(dims) if x != 'input_ids' else torch.randint(0, model.config.vocab_size, dims) for x in inputs)\n",
        "\n",
        "  hasbert = hasattr(model, 'bert')\n",
        "  print(f'hasbert={hasbert}')\n",
        "  model = model.bert if hasbert else model\n",
        "  # TODO: figure out how to do first nth encoder layers for non-awesome-align bert\n",
        "  model = ExportNthLayer(model, align_layer) if USE_AWESOME_ALIGN and align_layer is not None else model\n",
        "  # Export the model to ONNX\n",
        "  torch.onnx.export(\n",
        "      model,\n",
        "      inputs_ones, #(input_ids, attention_mask),\n",
        "      onnx_file_path,\n",
        "      export_params=True,\n",
        "      opset_version=opset_version,\n",
        "      do_constant_folding=True,\n",
        "      input_names = inputs,\n",
        "      output_names = outputs,\n",
        "      dynamic_axes=dynamic_axes,\n",
        "  )\n",
        "\n",
        "  if return_tensor_names:\n",
        "    om = onnx.load(onnx_file_path)\n",
        "\n",
        "    print('initializers')\n",
        "    for node in om.graph.initializer:\n",
        "      print(f'{node.name}')\n",
        "    return list(onnx_helper.enumerate_model_node_outputs(om))\n",
        "  else:\n",
        "    return f\"Model exported to {onnx_file_path}\"\n"
      ],
      "metadata": {
        "id": "-mUNmubaq4UQ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DO_ONNX_EXPORT=True\n",
        "\n",
        "def onnxpathm(x):\n",
        "  return f'{model_name}-maxlayer{x}.onnx'\n",
        "align_layer_max=9\n",
        "onnxpath=onnxpathm(align_layer_max)\n",
        "if DO_ONNX_EXPORT:\n",
        "    #onnxruntime.python.tools.transformers.export_onnx_model_from_pt(...)\n",
        "  if False and USE_AWESOME_ALIGN:\n",
        "    !CUDA_VISIBLE_DEVICES=0 PYTHONPATH=/content/awesome-align python /content/awesome-align/run_align.py --model_name_or_path=bert-base-multilingual-cased --output_onnx=$onnxpath --max_layer=$align_layer_max\n",
        "  else:\n",
        "    for x in to_onnx(model, onnxpath, align_layer=align_layer_max): print(str(x))\n",
        "  !du -h $onnxpath\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznxxf0jKu8P",
        "outputId": "17382ef3-5c0f-4acc-c32a-adeef97c43d4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hasbert=True\n",
            "ModuleList(\n",
            "  (0-8): 9 x BertLayer(\n",
            "    (attention): BertAttention(\n",
            "      (self): BertSelfAttention(\n",
            "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (output): BertSelfOutput(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (intermediate): BertIntermediate(\n",
            "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    )\n",
            "    (output): BertOutput(\n",
            "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "initializers\n",
            "bert.embeddings.word_embeddings.weight\n",
            "bert.embeddings.position_embeddings.weight\n",
            "bert.embeddings.token_type_embeddings.weight\n",
            "bert.embeddings.LayerNorm.weight\n",
            "bert.embeddings.LayerNorm.bias\n",
            "bert.encoder.layer.0.attention.self.query.bias\n",
            "bert.encoder.layer.0.attention.self.key.bias\n",
            "bert.encoder.layer.0.attention.self.value.bias\n",
            "bert.encoder.layer.0.attention.output.dense.bias\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.0.intermediate.dense.bias\n",
            "bert.encoder.layer.0.output.dense.bias\n",
            "bert.encoder.layer.0.output.LayerNorm.weight\n",
            "bert.encoder.layer.0.output.LayerNorm.bias\n",
            "bert.encoder.layer.1.attention.self.query.bias\n",
            "bert.encoder.layer.1.attention.self.key.bias\n",
            "bert.encoder.layer.1.attention.self.value.bias\n",
            "bert.encoder.layer.1.attention.output.dense.bias\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.1.intermediate.dense.bias\n",
            "bert.encoder.layer.1.output.dense.bias\n",
            "bert.encoder.layer.1.output.LayerNorm.weight\n",
            "bert.encoder.layer.1.output.LayerNorm.bias\n",
            "bert.encoder.layer.2.attention.self.query.bias\n",
            "bert.encoder.layer.2.attention.self.key.bias\n",
            "bert.encoder.layer.2.attention.self.value.bias\n",
            "bert.encoder.layer.2.attention.output.dense.bias\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.2.intermediate.dense.bias\n",
            "bert.encoder.layer.2.output.dense.bias\n",
            "bert.encoder.layer.2.output.LayerNorm.weight\n",
            "bert.encoder.layer.2.output.LayerNorm.bias\n",
            "bert.encoder.layer.3.attention.self.query.bias\n",
            "bert.encoder.layer.3.attention.self.key.bias\n",
            "bert.encoder.layer.3.attention.self.value.bias\n",
            "bert.encoder.layer.3.attention.output.dense.bias\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.3.intermediate.dense.bias\n",
            "bert.encoder.layer.3.output.dense.bias\n",
            "bert.encoder.layer.3.output.LayerNorm.weight\n",
            "bert.encoder.layer.3.output.LayerNorm.bias\n",
            "bert.encoder.layer.4.attention.self.query.bias\n",
            "bert.encoder.layer.4.attention.self.key.bias\n",
            "bert.encoder.layer.4.attention.self.value.bias\n",
            "bert.encoder.layer.4.attention.output.dense.bias\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.4.intermediate.dense.bias\n",
            "bert.encoder.layer.4.output.dense.bias\n",
            "bert.encoder.layer.4.output.LayerNorm.weight\n",
            "bert.encoder.layer.4.output.LayerNorm.bias\n",
            "bert.encoder.layer.5.attention.self.query.bias\n",
            "bert.encoder.layer.5.attention.self.key.bias\n",
            "bert.encoder.layer.5.attention.self.value.bias\n",
            "bert.encoder.layer.5.attention.output.dense.bias\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.5.intermediate.dense.bias\n",
            "bert.encoder.layer.5.output.dense.bias\n",
            "bert.encoder.layer.5.output.LayerNorm.weight\n",
            "bert.encoder.layer.5.output.LayerNorm.bias\n",
            "bert.encoder.layer.6.attention.self.query.bias\n",
            "bert.encoder.layer.6.attention.self.key.bias\n",
            "bert.encoder.layer.6.attention.self.value.bias\n",
            "bert.encoder.layer.6.attention.output.dense.bias\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.6.intermediate.dense.bias\n",
            "bert.encoder.layer.6.output.dense.bias\n",
            "bert.encoder.layer.6.output.LayerNorm.weight\n",
            "bert.encoder.layer.6.output.LayerNorm.bias\n",
            "bert.encoder.layer.7.attention.self.query.bias\n",
            "bert.encoder.layer.7.attention.self.key.bias\n",
            "bert.encoder.layer.7.attention.self.value.bias\n",
            "bert.encoder.layer.7.attention.output.dense.bias\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.7.intermediate.dense.bias\n",
            "bert.encoder.layer.7.output.dense.bias\n",
            "bert.encoder.layer.7.output.LayerNorm.weight\n",
            "bert.encoder.layer.7.output.LayerNorm.bias\n",
            "bert.encoder.layer.8.attention.self.query.bias\n",
            "bert.encoder.layer.8.attention.self.key.bias\n",
            "bert.encoder.layer.8.attention.self.value.bias\n",
            "bert.encoder.layer.8.attention.output.dense.bias\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.8.intermediate.dense.bias\n",
            "bert.encoder.layer.8.output.dense.bias\n",
            "bert.encoder.layer.8.output.LayerNorm.weight\n",
            "bert.encoder.layer.8.output.LayerNorm.bias\n",
            "onnx::MatMul_1357\n",
            "onnx::MatMul_1358\n",
            "onnx::MatMul_1359\n",
            "onnx::MatMul_1367\n",
            "onnx::MatMul_1368\n",
            "onnx::MatMul_1369\n",
            "onnx::MatMul_1370\n",
            "onnx::MatMul_1371\n",
            "onnx::MatMul_1372\n",
            "onnx::MatMul_1380\n",
            "onnx::MatMul_1381\n",
            "onnx::MatMul_1382\n",
            "onnx::MatMul_1383\n",
            "onnx::MatMul_1384\n",
            "onnx::MatMul_1385\n",
            "onnx::MatMul_1393\n",
            "onnx::MatMul_1394\n",
            "onnx::MatMul_1395\n",
            "onnx::MatMul_1396\n",
            "onnx::MatMul_1397\n",
            "onnx::MatMul_1398\n",
            "onnx::MatMul_1406\n",
            "onnx::MatMul_1407\n",
            "onnx::MatMul_1408\n",
            "onnx::MatMul_1409\n",
            "onnx::MatMul_1410\n",
            "onnx::MatMul_1411\n",
            "onnx::MatMul_1419\n",
            "onnx::MatMul_1420\n",
            "onnx::MatMul_1421\n",
            "onnx::MatMul_1422\n",
            "onnx::MatMul_1423\n",
            "onnx::MatMul_1424\n",
            "onnx::MatMul_1432\n",
            "onnx::MatMul_1433\n",
            "onnx::MatMul_1434\n",
            "onnx::MatMul_1435\n",
            "onnx::MatMul_1436\n",
            "onnx::MatMul_1437\n",
            "onnx::MatMul_1445\n",
            "onnx::MatMul_1446\n",
            "onnx::MatMul_1447\n",
            "onnx::MatMul_1448\n",
            "onnx::MatMul_1449\n",
            "onnx::MatMul_1450\n",
            "onnx::MatMul_1458\n",
            "onnx::MatMul_1459\n",
            "onnx::MatMul_1460\n",
            "onnx::MatMul_1461\n",
            "onnx::MatMul_1462\n",
            "onnx::MatMul_1463\n",
            "onnx::MatMul_1471\n",
            "onnx::MatMul_1472\n",
            "onnx::MatMul_1473\n",
            "/Constant_output_0\n",
            "/Unsqueeze_output_0\n",
            "/Constant_1_output_0\n",
            "/Unsqueeze_1_output_0\n",
            "/Cast_output_0\n",
            "/Constant_2_output_0\n",
            "/Sub_output_0\n",
            "/Constant_3_output_0\n",
            "/Mul_output_0\n",
            "/Shape_output_0\n",
            "/Constant_4_output_0\n",
            "/Gather_output_0\n",
            "/Shape_1_output_0\n",
            "/Constant_5_output_0\n",
            "/Gather_1_output_0\n",
            "onnx::Unsqueeze_214\n",
            "/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_216\n",
            "/Unsqueeze_3_output_0\n",
            "/Concat_output_0\n",
            "/ConstantOfShape_output_0\n",
            "/embeddings/Cast_output_0\n",
            "/embeddings/Constant_output_0\n",
            "/embeddings/Constant_1_output_0\n",
            "/embeddings/Range_output_0\n",
            "/embeddings/Constant_2_output_0\n",
            "/embeddings/Unsqueeze_output_0\n",
            "/embeddings/Constant_3_output_0\n",
            "/embeddings/Unsqueeze_1_output_0\n",
            "/embeddings/Constant_4_output_0\n",
            "/embeddings/Unsqueeze_2_output_0\n",
            "/embeddings/Concat_output_0\n",
            "/embeddings/Constant_5_output_0\n",
            "/embeddings/Reshape_output_0\n",
            "/embeddings/Shape_output_0\n",
            "/embeddings/ConstantOfShape_output_0\n",
            "/embeddings/Constant_6_output_0\n",
            "/embeddings/Mul_output_0\n",
            "/embeddings/Equal_output_0\n",
            "/embeddings/Where_output_0\n",
            "/embeddings/Expand_output_0\n",
            "/embeddings/word_embeddings/Gather_output_0\n",
            "/embeddings/position_embeddings/Gather_output_0\n",
            "/embeddings/token_type_embeddings/Gather_output_0\n",
            "/embeddings/Add_output_0\n",
            "/embeddings/Add_1_output_0\n",
            "/embeddings/LayerNorm/ReduceMean_output_0\n",
            "/embeddings/LayerNorm/Sub_output_0\n",
            "/embeddings/LayerNorm/Constant_output_0\n",
            "/embeddings/LayerNorm/Pow_output_0\n",
            "/embeddings/LayerNorm/ReduceMean_1_output_0\n",
            "/embeddings/LayerNorm/Constant_1_output_0\n",
            "/embeddings/LayerNorm/Add_output_0\n",
            "/embeddings/LayerNorm/Sqrt_output_0\n",
            "/embeddings/LayerNorm/Div_output_0\n",
            "/embeddings/LayerNorm/Mul_output_0\n",
            "/embeddings/LayerNorm/Add_1_output_0\n",
            "/layer.0/attention/self/query/MatMul_output_0\n",
            "/layer.0/attention/self/query/Add_output_0\n",
            "/layer.0/attention/self/key/MatMul_output_0\n",
            "/layer.0/attention/self/key/Add_output_0\n",
            "/layer.0/attention/self/value/MatMul_output_0\n",
            "/layer.0/attention/self/value/Add_output_0\n",
            "/layer.0/attention/self/Shape_output_0\n",
            "/layer.0/attention/self/Constant_output_0\n",
            "/layer.0/attention/self/Gather_output_0\n",
            "/layer.0/attention/self/Shape_1_output_0\n",
            "/layer.0/attention/self/Constant_1_output_0\n",
            "/layer.0/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_273\n",
            "/layer.0/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_275\n",
            "/layer.0/attention/self/Unsqueeze_1_output_0\n",
            "/layer.0/attention/self/Constant_2_output_0\n",
            "/layer.0/attention/self/Constant_3_output_0\n",
            "/layer.0/attention/self/Concat_output_0\n",
            "/layer.0/attention/self/Reshape_output_0\n",
            "/layer.0/attention/self/Transpose_output_0\n",
            "/layer.0/attention/self/Shape_2_output_0\n",
            "/layer.0/attention/self/Constant_4_output_0\n",
            "/layer.0/attention/self/Gather_2_output_0\n",
            "/layer.0/attention/self/Shape_3_output_0\n",
            "/layer.0/attention/self/Constant_5_output_0\n",
            "/layer.0/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_290\n",
            "/layer.0/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_292\n",
            "/layer.0/attention/self/Unsqueeze_3_output_0\n",
            "/layer.0/attention/self/Constant_6_output_0\n",
            "/layer.0/attention/self/Constant_7_output_0\n",
            "/layer.0/attention/self/Concat_1_output_0\n",
            "/layer.0/attention/self/Reshape_1_output_0\n",
            "/layer.0/attention/self/Shape_4_output_0\n",
            "/layer.0/attention/self/Constant_8_output_0\n",
            "/layer.0/attention/self/Gather_4_output_0\n",
            "/layer.0/attention/self/Shape_5_output_0\n",
            "/layer.0/attention/self/Constant_9_output_0\n",
            "/layer.0/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_306\n",
            "/layer.0/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_308\n",
            "/layer.0/attention/self/Unsqueeze_5_output_0\n",
            "/layer.0/attention/self/Constant_10_output_0\n",
            "/layer.0/attention/self/Constant_11_output_0\n",
            "/layer.0/attention/self/Concat_2_output_0\n",
            "/layer.0/attention/self/Reshape_2_output_0\n",
            "/layer.0/attention/self/Transpose_1_output_0\n",
            "/layer.0/attention/self/Transpose_2_output_0\n",
            "/layer.0/attention/self/MatMul_output_0\n",
            "/layer.0/attention/self/Constant_12_output_0\n",
            "/layer.0/attention/self/Div_output_0\n",
            "/layer.0/attention/self/Add_output_0\n",
            "/layer.0/attention/self/Softmax_output_0\n",
            "/layer.0/attention/self/MatMul_1_output_0\n",
            "/layer.0/attention/self/Transpose_3_output_0\n",
            "/layer.0/attention/self/Shape_6_output_0\n",
            "/layer.0/attention/self/Constant_13_output_0\n",
            "/layer.0/attention/self/Gather_6_output_0\n",
            "/layer.0/attention/self/Shape_7_output_0\n",
            "/layer.0/attention/self/Constant_14_output_0\n",
            "/layer.0/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_332\n",
            "/layer.0/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_334\n",
            "/layer.0/attention/self/Unsqueeze_7_output_0\n",
            "/layer.0/attention/self/Constant_15_output_0\n",
            "/layer.0/attention/self/Concat_3_output_0\n",
            "/layer.0/attention/self/Reshape_3_output_0\n",
            "/layer.0/attention/output/dense/MatMul_output_0\n",
            "/layer.0/attention/output/dense/Add_output_0\n",
            "/layer.0/attention/output/Add_output_0\n",
            "/layer.0/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.0/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.0/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.0/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.0/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.0/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.0/attention/output/LayerNorm/Add_output_0\n",
            "/layer.0/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.0/attention/output/LayerNorm/Div_output_0\n",
            "/layer.0/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.0/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.0/intermediate/dense/MatMul_output_0\n",
            "/layer.0/intermediate/dense/Add_output_0\n",
            "/layer.0/intermediate/Constant_output_0\n",
            "/layer.0/intermediate/Div_output_0\n",
            "/layer.0/intermediate/Erf_output_0\n",
            "/layer.0/intermediate/Constant_1_output_0\n",
            "/layer.0/intermediate/Add_output_0\n",
            "/layer.0/intermediate/Mul_output_0\n",
            "/layer.0/intermediate/Constant_2_output_0\n",
            "/layer.0/intermediate/Mul_1_output_0\n",
            "/layer.0/output/dense/MatMul_output_0\n",
            "/layer.0/output/dense/Add_output_0\n",
            "/layer.0/output/Add_output_0\n",
            "/layer.0/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.0/output/LayerNorm/Sub_output_0\n",
            "/layer.0/output/LayerNorm/Constant_output_0\n",
            "/layer.0/output/LayerNorm/Pow_output_0\n",
            "/layer.0/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.0/output/LayerNorm/Constant_1_output_0\n",
            "/layer.0/output/LayerNorm/Add_output_0\n",
            "/layer.0/output/LayerNorm/Sqrt_output_0\n",
            "/layer.0/output/LayerNorm/Div_output_0\n",
            "/layer.0/output/LayerNorm/Mul_output_0\n",
            "/layer.0/output/LayerNorm/Add_1_output_0\n",
            "/layer.1/attention/self/query/MatMul_output_0\n",
            "/layer.1/attention/self/query/Add_output_0\n",
            "/layer.1/attention/self/key/MatMul_output_0\n",
            "/layer.1/attention/self/key/Add_output_0\n",
            "/layer.1/attention/self/value/MatMul_output_0\n",
            "/layer.1/attention/self/value/Add_output_0\n",
            "/layer.1/attention/self/Shape_output_0\n",
            "/layer.1/attention/self/Constant_output_0\n",
            "/layer.1/attention/self/Gather_output_0\n",
            "/layer.1/attention/self/Shape_1_output_0\n",
            "/layer.1/attention/self/Constant_1_output_0\n",
            "/layer.1/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_396\n",
            "/layer.1/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_398\n",
            "/layer.1/attention/self/Unsqueeze_1_output_0\n",
            "/layer.1/attention/self/Constant_2_output_0\n",
            "/layer.1/attention/self/Constant_3_output_0\n",
            "/layer.1/attention/self/Concat_output_0\n",
            "/layer.1/attention/self/Reshape_output_0\n",
            "/layer.1/attention/self/Transpose_output_0\n",
            "/layer.1/attention/self/Shape_2_output_0\n",
            "/layer.1/attention/self/Constant_4_output_0\n",
            "/layer.1/attention/self/Gather_2_output_0\n",
            "/layer.1/attention/self/Shape_3_output_0\n",
            "/layer.1/attention/self/Constant_5_output_0\n",
            "/layer.1/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_413\n",
            "/layer.1/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_415\n",
            "/layer.1/attention/self/Unsqueeze_3_output_0\n",
            "/layer.1/attention/self/Constant_6_output_0\n",
            "/layer.1/attention/self/Constant_7_output_0\n",
            "/layer.1/attention/self/Concat_1_output_0\n",
            "/layer.1/attention/self/Reshape_1_output_0\n",
            "/layer.1/attention/self/Shape_4_output_0\n",
            "/layer.1/attention/self/Constant_8_output_0\n",
            "/layer.1/attention/self/Gather_4_output_0\n",
            "/layer.1/attention/self/Shape_5_output_0\n",
            "/layer.1/attention/self/Constant_9_output_0\n",
            "/layer.1/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_429\n",
            "/layer.1/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_431\n",
            "/layer.1/attention/self/Unsqueeze_5_output_0\n",
            "/layer.1/attention/self/Constant_10_output_0\n",
            "/layer.1/attention/self/Constant_11_output_0\n",
            "/layer.1/attention/self/Concat_2_output_0\n",
            "/layer.1/attention/self/Reshape_2_output_0\n",
            "/layer.1/attention/self/Transpose_1_output_0\n",
            "/layer.1/attention/self/Transpose_2_output_0\n",
            "/layer.1/attention/self/MatMul_output_0\n",
            "/layer.1/attention/self/Constant_12_output_0\n",
            "/layer.1/attention/self/Div_output_0\n",
            "/layer.1/attention/self/Add_output_0\n",
            "/layer.1/attention/self/Softmax_output_0\n",
            "/layer.1/attention/self/MatMul_1_output_0\n",
            "/layer.1/attention/self/Transpose_3_output_0\n",
            "/layer.1/attention/self/Shape_6_output_0\n",
            "/layer.1/attention/self/Constant_13_output_0\n",
            "/layer.1/attention/self/Gather_6_output_0\n",
            "/layer.1/attention/self/Shape_7_output_0\n",
            "/layer.1/attention/self/Constant_14_output_0\n",
            "/layer.1/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_454\n",
            "/layer.1/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_456\n",
            "/layer.1/attention/self/Unsqueeze_7_output_0\n",
            "/layer.1/attention/self/Constant_15_output_0\n",
            "/layer.1/attention/self/Concat_3_output_0\n",
            "/layer.1/attention/self/Reshape_3_output_0\n",
            "/layer.1/attention/output/dense/MatMul_output_0\n",
            "/layer.1/attention/output/dense/Add_output_0\n",
            "/layer.1/attention/output/Add_output_0\n",
            "/layer.1/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.1/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.1/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.1/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.1/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.1/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.1/attention/output/LayerNorm/Add_output_0\n",
            "/layer.1/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.1/attention/output/LayerNorm/Div_output_0\n",
            "/layer.1/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.1/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.1/intermediate/dense/MatMul_output_0\n",
            "/layer.1/intermediate/dense/Add_output_0\n",
            "/layer.1/intermediate/Constant_output_0\n",
            "/layer.1/intermediate/Div_output_0\n",
            "/layer.1/intermediate/Erf_output_0\n",
            "/layer.1/intermediate/Constant_1_output_0\n",
            "/layer.1/intermediate/Add_output_0\n",
            "/layer.1/intermediate/Mul_output_0\n",
            "/layer.1/intermediate/Constant_2_output_0\n",
            "/layer.1/intermediate/Mul_1_output_0\n",
            "/layer.1/output/dense/MatMul_output_0\n",
            "/layer.1/output/dense/Add_output_0\n",
            "/layer.1/output/Add_output_0\n",
            "/layer.1/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.1/output/LayerNorm/Sub_output_0\n",
            "/layer.1/output/LayerNorm/Constant_output_0\n",
            "/layer.1/output/LayerNorm/Pow_output_0\n",
            "/layer.1/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.1/output/LayerNorm/Constant_1_output_0\n",
            "/layer.1/output/LayerNorm/Add_output_0\n",
            "/layer.1/output/LayerNorm/Sqrt_output_0\n",
            "/layer.1/output/LayerNorm/Div_output_0\n",
            "/layer.1/output/LayerNorm/Mul_output_0\n",
            "/layer.1/output/LayerNorm/Add_1_output_0\n",
            "/layer.2/attention/self/query/MatMul_output_0\n",
            "/layer.2/attention/self/query/Add_output_0\n",
            "/layer.2/attention/self/key/MatMul_output_0\n",
            "/layer.2/attention/self/key/Add_output_0\n",
            "/layer.2/attention/self/value/MatMul_output_0\n",
            "/layer.2/attention/self/value/Add_output_0\n",
            "/layer.2/attention/self/Shape_output_0\n",
            "/layer.2/attention/self/Constant_output_0\n",
            "/layer.2/attention/self/Gather_output_0\n",
            "/layer.2/attention/self/Shape_1_output_0\n",
            "/layer.2/attention/self/Constant_1_output_0\n",
            "/layer.2/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_518\n",
            "/layer.2/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_520\n",
            "/layer.2/attention/self/Unsqueeze_1_output_0\n",
            "/layer.2/attention/self/Constant_2_output_0\n",
            "/layer.2/attention/self/Constant_3_output_0\n",
            "/layer.2/attention/self/Concat_output_0\n",
            "/layer.2/attention/self/Reshape_output_0\n",
            "/layer.2/attention/self/Transpose_output_0\n",
            "/layer.2/attention/self/Shape_2_output_0\n",
            "/layer.2/attention/self/Constant_4_output_0\n",
            "/layer.2/attention/self/Gather_2_output_0\n",
            "/layer.2/attention/self/Shape_3_output_0\n",
            "/layer.2/attention/self/Constant_5_output_0\n",
            "/layer.2/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_535\n",
            "/layer.2/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_537\n",
            "/layer.2/attention/self/Unsqueeze_3_output_0\n",
            "/layer.2/attention/self/Constant_6_output_0\n",
            "/layer.2/attention/self/Constant_7_output_0\n",
            "/layer.2/attention/self/Concat_1_output_0\n",
            "/layer.2/attention/self/Reshape_1_output_0\n",
            "/layer.2/attention/self/Shape_4_output_0\n",
            "/layer.2/attention/self/Constant_8_output_0\n",
            "/layer.2/attention/self/Gather_4_output_0\n",
            "/layer.2/attention/self/Shape_5_output_0\n",
            "/layer.2/attention/self/Constant_9_output_0\n",
            "/layer.2/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_551\n",
            "/layer.2/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_553\n",
            "/layer.2/attention/self/Unsqueeze_5_output_0\n",
            "/layer.2/attention/self/Constant_10_output_0\n",
            "/layer.2/attention/self/Constant_11_output_0\n",
            "/layer.2/attention/self/Concat_2_output_0\n",
            "/layer.2/attention/self/Reshape_2_output_0\n",
            "/layer.2/attention/self/Transpose_1_output_0\n",
            "/layer.2/attention/self/Transpose_2_output_0\n",
            "/layer.2/attention/self/MatMul_output_0\n",
            "/layer.2/attention/self/Constant_12_output_0\n",
            "/layer.2/attention/self/Div_output_0\n",
            "/layer.2/attention/self/Add_output_0\n",
            "/layer.2/attention/self/Softmax_output_0\n",
            "/layer.2/attention/self/MatMul_1_output_0\n",
            "/layer.2/attention/self/Transpose_3_output_0\n",
            "/layer.2/attention/self/Shape_6_output_0\n",
            "/layer.2/attention/self/Constant_13_output_0\n",
            "/layer.2/attention/self/Gather_6_output_0\n",
            "/layer.2/attention/self/Shape_7_output_0\n",
            "/layer.2/attention/self/Constant_14_output_0\n",
            "/layer.2/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_576\n",
            "/layer.2/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_578\n",
            "/layer.2/attention/self/Unsqueeze_7_output_0\n",
            "/layer.2/attention/self/Constant_15_output_0\n",
            "/layer.2/attention/self/Concat_3_output_0\n",
            "/layer.2/attention/self/Reshape_3_output_0\n",
            "/layer.2/attention/output/dense/MatMul_output_0\n",
            "/layer.2/attention/output/dense/Add_output_0\n",
            "/layer.2/attention/output/Add_output_0\n",
            "/layer.2/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.2/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.2/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.2/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.2/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.2/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.2/attention/output/LayerNorm/Add_output_0\n",
            "/layer.2/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.2/attention/output/LayerNorm/Div_output_0\n",
            "/layer.2/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.2/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.2/intermediate/dense/MatMul_output_0\n",
            "/layer.2/intermediate/dense/Add_output_0\n",
            "/layer.2/intermediate/Constant_output_0\n",
            "/layer.2/intermediate/Div_output_0\n",
            "/layer.2/intermediate/Erf_output_0\n",
            "/layer.2/intermediate/Constant_1_output_0\n",
            "/layer.2/intermediate/Add_output_0\n",
            "/layer.2/intermediate/Mul_output_0\n",
            "/layer.2/intermediate/Constant_2_output_0\n",
            "/layer.2/intermediate/Mul_1_output_0\n",
            "/layer.2/output/dense/MatMul_output_0\n",
            "/layer.2/output/dense/Add_output_0\n",
            "/layer.2/output/Add_output_0\n",
            "/layer.2/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.2/output/LayerNorm/Sub_output_0\n",
            "/layer.2/output/LayerNorm/Constant_output_0\n",
            "/layer.2/output/LayerNorm/Pow_output_0\n",
            "/layer.2/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.2/output/LayerNorm/Constant_1_output_0\n",
            "/layer.2/output/LayerNorm/Add_output_0\n",
            "/layer.2/output/LayerNorm/Sqrt_output_0\n",
            "/layer.2/output/LayerNorm/Div_output_0\n",
            "/layer.2/output/LayerNorm/Mul_output_0\n",
            "/layer.2/output/LayerNorm/Add_1_output_0\n",
            "/layer.3/attention/self/query/MatMul_output_0\n",
            "/layer.3/attention/self/query/Add_output_0\n",
            "/layer.3/attention/self/key/MatMul_output_0\n",
            "/layer.3/attention/self/key/Add_output_0\n",
            "/layer.3/attention/self/value/MatMul_output_0\n",
            "/layer.3/attention/self/value/Add_output_0\n",
            "/layer.3/attention/self/Shape_output_0\n",
            "/layer.3/attention/self/Constant_output_0\n",
            "/layer.3/attention/self/Gather_output_0\n",
            "/layer.3/attention/self/Shape_1_output_0\n",
            "/layer.3/attention/self/Constant_1_output_0\n",
            "/layer.3/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_640\n",
            "/layer.3/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_642\n",
            "/layer.3/attention/self/Unsqueeze_1_output_0\n",
            "/layer.3/attention/self/Constant_2_output_0\n",
            "/layer.3/attention/self/Constant_3_output_0\n",
            "/layer.3/attention/self/Concat_output_0\n",
            "/layer.3/attention/self/Reshape_output_0\n",
            "/layer.3/attention/self/Transpose_output_0\n",
            "/layer.3/attention/self/Shape_2_output_0\n",
            "/layer.3/attention/self/Constant_4_output_0\n",
            "/layer.3/attention/self/Gather_2_output_0\n",
            "/layer.3/attention/self/Shape_3_output_0\n",
            "/layer.3/attention/self/Constant_5_output_0\n",
            "/layer.3/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_657\n",
            "/layer.3/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_659\n",
            "/layer.3/attention/self/Unsqueeze_3_output_0\n",
            "/layer.3/attention/self/Constant_6_output_0\n",
            "/layer.3/attention/self/Constant_7_output_0\n",
            "/layer.3/attention/self/Concat_1_output_0\n",
            "/layer.3/attention/self/Reshape_1_output_0\n",
            "/layer.3/attention/self/Shape_4_output_0\n",
            "/layer.3/attention/self/Constant_8_output_0\n",
            "/layer.3/attention/self/Gather_4_output_0\n",
            "/layer.3/attention/self/Shape_5_output_0\n",
            "/layer.3/attention/self/Constant_9_output_0\n",
            "/layer.3/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_673\n",
            "/layer.3/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_675\n",
            "/layer.3/attention/self/Unsqueeze_5_output_0\n",
            "/layer.3/attention/self/Constant_10_output_0\n",
            "/layer.3/attention/self/Constant_11_output_0\n",
            "/layer.3/attention/self/Concat_2_output_0\n",
            "/layer.3/attention/self/Reshape_2_output_0\n",
            "/layer.3/attention/self/Transpose_1_output_0\n",
            "/layer.3/attention/self/Transpose_2_output_0\n",
            "/layer.3/attention/self/MatMul_output_0\n",
            "/layer.3/attention/self/Constant_12_output_0\n",
            "/layer.3/attention/self/Div_output_0\n",
            "/layer.3/attention/self/Add_output_0\n",
            "/layer.3/attention/self/Softmax_output_0\n",
            "/layer.3/attention/self/MatMul_1_output_0\n",
            "/layer.3/attention/self/Transpose_3_output_0\n",
            "/layer.3/attention/self/Shape_6_output_0\n",
            "/layer.3/attention/self/Constant_13_output_0\n",
            "/layer.3/attention/self/Gather_6_output_0\n",
            "/layer.3/attention/self/Shape_7_output_0\n",
            "/layer.3/attention/self/Constant_14_output_0\n",
            "/layer.3/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_698\n",
            "/layer.3/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_700\n",
            "/layer.3/attention/self/Unsqueeze_7_output_0\n",
            "/layer.3/attention/self/Constant_15_output_0\n",
            "/layer.3/attention/self/Concat_3_output_0\n",
            "/layer.3/attention/self/Reshape_3_output_0\n",
            "/layer.3/attention/output/dense/MatMul_output_0\n",
            "/layer.3/attention/output/dense/Add_output_0\n",
            "/layer.3/attention/output/Add_output_0\n",
            "/layer.3/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.3/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.3/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.3/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.3/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.3/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.3/attention/output/LayerNorm/Add_output_0\n",
            "/layer.3/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.3/attention/output/LayerNorm/Div_output_0\n",
            "/layer.3/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.3/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.3/intermediate/dense/MatMul_output_0\n",
            "/layer.3/intermediate/dense/Add_output_0\n",
            "/layer.3/intermediate/Constant_output_0\n",
            "/layer.3/intermediate/Div_output_0\n",
            "/layer.3/intermediate/Erf_output_0\n",
            "/layer.3/intermediate/Constant_1_output_0\n",
            "/layer.3/intermediate/Add_output_0\n",
            "/layer.3/intermediate/Mul_output_0\n",
            "/layer.3/intermediate/Constant_2_output_0\n",
            "/layer.3/intermediate/Mul_1_output_0\n",
            "/layer.3/output/dense/MatMul_output_0\n",
            "/layer.3/output/dense/Add_output_0\n",
            "/layer.3/output/Add_output_0\n",
            "/layer.3/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.3/output/LayerNorm/Sub_output_0\n",
            "/layer.3/output/LayerNorm/Constant_output_0\n",
            "/layer.3/output/LayerNorm/Pow_output_0\n",
            "/layer.3/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.3/output/LayerNorm/Constant_1_output_0\n",
            "/layer.3/output/LayerNorm/Add_output_0\n",
            "/layer.3/output/LayerNorm/Sqrt_output_0\n",
            "/layer.3/output/LayerNorm/Div_output_0\n",
            "/layer.3/output/LayerNorm/Mul_output_0\n",
            "/layer.3/output/LayerNorm/Add_1_output_0\n",
            "/layer.4/attention/self/query/MatMul_output_0\n",
            "/layer.4/attention/self/query/Add_output_0\n",
            "/layer.4/attention/self/key/MatMul_output_0\n",
            "/layer.4/attention/self/key/Add_output_0\n",
            "/layer.4/attention/self/value/MatMul_output_0\n",
            "/layer.4/attention/self/value/Add_output_0\n",
            "/layer.4/attention/self/Shape_output_0\n",
            "/layer.4/attention/self/Constant_output_0\n",
            "/layer.4/attention/self/Gather_output_0\n",
            "/layer.4/attention/self/Shape_1_output_0\n",
            "/layer.4/attention/self/Constant_1_output_0\n",
            "/layer.4/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_762\n",
            "/layer.4/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_764\n",
            "/layer.4/attention/self/Unsqueeze_1_output_0\n",
            "/layer.4/attention/self/Constant_2_output_0\n",
            "/layer.4/attention/self/Constant_3_output_0\n",
            "/layer.4/attention/self/Concat_output_0\n",
            "/layer.4/attention/self/Reshape_output_0\n",
            "/layer.4/attention/self/Transpose_output_0\n",
            "/layer.4/attention/self/Shape_2_output_0\n",
            "/layer.4/attention/self/Constant_4_output_0\n",
            "/layer.4/attention/self/Gather_2_output_0\n",
            "/layer.4/attention/self/Shape_3_output_0\n",
            "/layer.4/attention/self/Constant_5_output_0\n",
            "/layer.4/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_779\n",
            "/layer.4/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_781\n",
            "/layer.4/attention/self/Unsqueeze_3_output_0\n",
            "/layer.4/attention/self/Constant_6_output_0\n",
            "/layer.4/attention/self/Constant_7_output_0\n",
            "/layer.4/attention/self/Concat_1_output_0\n",
            "/layer.4/attention/self/Reshape_1_output_0\n",
            "/layer.4/attention/self/Shape_4_output_0\n",
            "/layer.4/attention/self/Constant_8_output_0\n",
            "/layer.4/attention/self/Gather_4_output_0\n",
            "/layer.4/attention/self/Shape_5_output_0\n",
            "/layer.4/attention/self/Constant_9_output_0\n",
            "/layer.4/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_795\n",
            "/layer.4/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_797\n",
            "/layer.4/attention/self/Unsqueeze_5_output_0\n",
            "/layer.4/attention/self/Constant_10_output_0\n",
            "/layer.4/attention/self/Constant_11_output_0\n",
            "/layer.4/attention/self/Concat_2_output_0\n",
            "/layer.4/attention/self/Reshape_2_output_0\n",
            "/layer.4/attention/self/Transpose_1_output_0\n",
            "/layer.4/attention/self/Transpose_2_output_0\n",
            "/layer.4/attention/self/MatMul_output_0\n",
            "/layer.4/attention/self/Constant_12_output_0\n",
            "/layer.4/attention/self/Div_output_0\n",
            "/layer.4/attention/self/Add_output_0\n",
            "/layer.4/attention/self/Softmax_output_0\n",
            "/layer.4/attention/self/MatMul_1_output_0\n",
            "/layer.4/attention/self/Transpose_3_output_0\n",
            "/layer.4/attention/self/Shape_6_output_0\n",
            "/layer.4/attention/self/Constant_13_output_0\n",
            "/layer.4/attention/self/Gather_6_output_0\n",
            "/layer.4/attention/self/Shape_7_output_0\n",
            "/layer.4/attention/self/Constant_14_output_0\n",
            "/layer.4/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_820\n",
            "/layer.4/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_822\n",
            "/layer.4/attention/self/Unsqueeze_7_output_0\n",
            "/layer.4/attention/self/Constant_15_output_0\n",
            "/layer.4/attention/self/Concat_3_output_0\n",
            "/layer.4/attention/self/Reshape_3_output_0\n",
            "/layer.4/attention/output/dense/MatMul_output_0\n",
            "/layer.4/attention/output/dense/Add_output_0\n",
            "/layer.4/attention/output/Add_output_0\n",
            "/layer.4/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.4/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.4/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.4/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.4/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.4/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.4/attention/output/LayerNorm/Add_output_0\n",
            "/layer.4/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.4/attention/output/LayerNorm/Div_output_0\n",
            "/layer.4/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.4/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.4/intermediate/dense/MatMul_output_0\n",
            "/layer.4/intermediate/dense/Add_output_0\n",
            "/layer.4/intermediate/Constant_output_0\n",
            "/layer.4/intermediate/Div_output_0\n",
            "/layer.4/intermediate/Erf_output_0\n",
            "/layer.4/intermediate/Constant_1_output_0\n",
            "/layer.4/intermediate/Add_output_0\n",
            "/layer.4/intermediate/Mul_output_0\n",
            "/layer.4/intermediate/Constant_2_output_0\n",
            "/layer.4/intermediate/Mul_1_output_0\n",
            "/layer.4/output/dense/MatMul_output_0\n",
            "/layer.4/output/dense/Add_output_0\n",
            "/layer.4/output/Add_output_0\n",
            "/layer.4/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.4/output/LayerNorm/Sub_output_0\n",
            "/layer.4/output/LayerNorm/Constant_output_0\n",
            "/layer.4/output/LayerNorm/Pow_output_0\n",
            "/layer.4/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.4/output/LayerNorm/Constant_1_output_0\n",
            "/layer.4/output/LayerNorm/Add_output_0\n",
            "/layer.4/output/LayerNorm/Sqrt_output_0\n",
            "/layer.4/output/LayerNorm/Div_output_0\n",
            "/layer.4/output/LayerNorm/Mul_output_0\n",
            "/layer.4/output/LayerNorm/Add_1_output_0\n",
            "/layer.5/attention/self/query/MatMul_output_0\n",
            "/layer.5/attention/self/query/Add_output_0\n",
            "/layer.5/attention/self/key/MatMul_output_0\n",
            "/layer.5/attention/self/key/Add_output_0\n",
            "/layer.5/attention/self/value/MatMul_output_0\n",
            "/layer.5/attention/self/value/Add_output_0\n",
            "/layer.5/attention/self/Shape_output_0\n",
            "/layer.5/attention/self/Constant_output_0\n",
            "/layer.5/attention/self/Gather_output_0\n",
            "/layer.5/attention/self/Shape_1_output_0\n",
            "/layer.5/attention/self/Constant_1_output_0\n",
            "/layer.5/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_884\n",
            "/layer.5/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_886\n",
            "/layer.5/attention/self/Unsqueeze_1_output_0\n",
            "/layer.5/attention/self/Constant_2_output_0\n",
            "/layer.5/attention/self/Constant_3_output_0\n",
            "/layer.5/attention/self/Concat_output_0\n",
            "/layer.5/attention/self/Reshape_output_0\n",
            "/layer.5/attention/self/Transpose_output_0\n",
            "/layer.5/attention/self/Shape_2_output_0\n",
            "/layer.5/attention/self/Constant_4_output_0\n",
            "/layer.5/attention/self/Gather_2_output_0\n",
            "/layer.5/attention/self/Shape_3_output_0\n",
            "/layer.5/attention/self/Constant_5_output_0\n",
            "/layer.5/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_901\n",
            "/layer.5/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_903\n",
            "/layer.5/attention/self/Unsqueeze_3_output_0\n",
            "/layer.5/attention/self/Constant_6_output_0\n",
            "/layer.5/attention/self/Constant_7_output_0\n",
            "/layer.5/attention/self/Concat_1_output_0\n",
            "/layer.5/attention/self/Reshape_1_output_0\n",
            "/layer.5/attention/self/Shape_4_output_0\n",
            "/layer.5/attention/self/Constant_8_output_0\n",
            "/layer.5/attention/self/Gather_4_output_0\n",
            "/layer.5/attention/self/Shape_5_output_0\n",
            "/layer.5/attention/self/Constant_9_output_0\n",
            "/layer.5/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_917\n",
            "/layer.5/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_919\n",
            "/layer.5/attention/self/Unsqueeze_5_output_0\n",
            "/layer.5/attention/self/Constant_10_output_0\n",
            "/layer.5/attention/self/Constant_11_output_0\n",
            "/layer.5/attention/self/Concat_2_output_0\n",
            "/layer.5/attention/self/Reshape_2_output_0\n",
            "/layer.5/attention/self/Transpose_1_output_0\n",
            "/layer.5/attention/self/Transpose_2_output_0\n",
            "/layer.5/attention/self/MatMul_output_0\n",
            "/layer.5/attention/self/Constant_12_output_0\n",
            "/layer.5/attention/self/Div_output_0\n",
            "/layer.5/attention/self/Add_output_0\n",
            "/layer.5/attention/self/Softmax_output_0\n",
            "/layer.5/attention/self/MatMul_1_output_0\n",
            "/layer.5/attention/self/Transpose_3_output_0\n",
            "/layer.5/attention/self/Shape_6_output_0\n",
            "/layer.5/attention/self/Constant_13_output_0\n",
            "/layer.5/attention/self/Gather_6_output_0\n",
            "/layer.5/attention/self/Shape_7_output_0\n",
            "/layer.5/attention/self/Constant_14_output_0\n",
            "/layer.5/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_942\n",
            "/layer.5/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_944\n",
            "/layer.5/attention/self/Unsqueeze_7_output_0\n",
            "/layer.5/attention/self/Constant_15_output_0\n",
            "/layer.5/attention/self/Concat_3_output_0\n",
            "/layer.5/attention/self/Reshape_3_output_0\n",
            "/layer.5/attention/output/dense/MatMul_output_0\n",
            "/layer.5/attention/output/dense/Add_output_0\n",
            "/layer.5/attention/output/Add_output_0\n",
            "/layer.5/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.5/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.5/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.5/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.5/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.5/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.5/attention/output/LayerNorm/Add_output_0\n",
            "/layer.5/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.5/attention/output/LayerNorm/Div_output_0\n",
            "/layer.5/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.5/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.5/intermediate/dense/MatMul_output_0\n",
            "/layer.5/intermediate/dense/Add_output_0\n",
            "/layer.5/intermediate/Constant_output_0\n",
            "/layer.5/intermediate/Div_output_0\n",
            "/layer.5/intermediate/Erf_output_0\n",
            "/layer.5/intermediate/Constant_1_output_0\n",
            "/layer.5/intermediate/Add_output_0\n",
            "/layer.5/intermediate/Mul_output_0\n",
            "/layer.5/intermediate/Constant_2_output_0\n",
            "/layer.5/intermediate/Mul_1_output_0\n",
            "/layer.5/output/dense/MatMul_output_0\n",
            "/layer.5/output/dense/Add_output_0\n",
            "/layer.5/output/Add_output_0\n",
            "/layer.5/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.5/output/LayerNorm/Sub_output_0\n",
            "/layer.5/output/LayerNorm/Constant_output_0\n",
            "/layer.5/output/LayerNorm/Pow_output_0\n",
            "/layer.5/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.5/output/LayerNorm/Constant_1_output_0\n",
            "/layer.5/output/LayerNorm/Add_output_0\n",
            "/layer.5/output/LayerNorm/Sqrt_output_0\n",
            "/layer.5/output/LayerNorm/Div_output_0\n",
            "/layer.5/output/LayerNorm/Mul_output_0\n",
            "/layer.5/output/LayerNorm/Add_1_output_0\n",
            "/layer.6/attention/self/query/MatMul_output_0\n",
            "/layer.6/attention/self/query/Add_output_0\n",
            "/layer.6/attention/self/key/MatMul_output_0\n",
            "/layer.6/attention/self/key/Add_output_0\n",
            "/layer.6/attention/self/value/MatMul_output_0\n",
            "/layer.6/attention/self/value/Add_output_0\n",
            "/layer.6/attention/self/Shape_output_0\n",
            "/layer.6/attention/self/Constant_output_0\n",
            "/layer.6/attention/self/Gather_output_0\n",
            "/layer.6/attention/self/Shape_1_output_0\n",
            "/layer.6/attention/self/Constant_1_output_0\n",
            "/layer.6/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_1006\n",
            "/layer.6/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_1008\n",
            "/layer.6/attention/self/Unsqueeze_1_output_0\n",
            "/layer.6/attention/self/Constant_2_output_0\n",
            "/layer.6/attention/self/Constant_3_output_0\n",
            "/layer.6/attention/self/Concat_output_0\n",
            "/layer.6/attention/self/Reshape_output_0\n",
            "/layer.6/attention/self/Transpose_output_0\n",
            "/layer.6/attention/self/Shape_2_output_0\n",
            "/layer.6/attention/self/Constant_4_output_0\n",
            "/layer.6/attention/self/Gather_2_output_0\n",
            "/layer.6/attention/self/Shape_3_output_0\n",
            "/layer.6/attention/self/Constant_5_output_0\n",
            "/layer.6/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_1023\n",
            "/layer.6/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_1025\n",
            "/layer.6/attention/self/Unsqueeze_3_output_0\n",
            "/layer.6/attention/self/Constant_6_output_0\n",
            "/layer.6/attention/self/Constant_7_output_0\n",
            "/layer.6/attention/self/Concat_1_output_0\n",
            "/layer.6/attention/self/Reshape_1_output_0\n",
            "/layer.6/attention/self/Shape_4_output_0\n",
            "/layer.6/attention/self/Constant_8_output_0\n",
            "/layer.6/attention/self/Gather_4_output_0\n",
            "/layer.6/attention/self/Shape_5_output_0\n",
            "/layer.6/attention/self/Constant_9_output_0\n",
            "/layer.6/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_1039\n",
            "/layer.6/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_1041\n",
            "/layer.6/attention/self/Unsqueeze_5_output_0\n",
            "/layer.6/attention/self/Constant_10_output_0\n",
            "/layer.6/attention/self/Constant_11_output_0\n",
            "/layer.6/attention/self/Concat_2_output_0\n",
            "/layer.6/attention/self/Reshape_2_output_0\n",
            "/layer.6/attention/self/Transpose_1_output_0\n",
            "/layer.6/attention/self/Transpose_2_output_0\n",
            "/layer.6/attention/self/MatMul_output_0\n",
            "/layer.6/attention/self/Constant_12_output_0\n",
            "/layer.6/attention/self/Div_output_0\n",
            "/layer.6/attention/self/Add_output_0\n",
            "/layer.6/attention/self/Softmax_output_0\n",
            "/layer.6/attention/self/MatMul_1_output_0\n",
            "/layer.6/attention/self/Transpose_3_output_0\n",
            "/layer.6/attention/self/Shape_6_output_0\n",
            "/layer.6/attention/self/Constant_13_output_0\n",
            "/layer.6/attention/self/Gather_6_output_0\n",
            "/layer.6/attention/self/Shape_7_output_0\n",
            "/layer.6/attention/self/Constant_14_output_0\n",
            "/layer.6/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_1064\n",
            "/layer.6/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_1066\n",
            "/layer.6/attention/self/Unsqueeze_7_output_0\n",
            "/layer.6/attention/self/Constant_15_output_0\n",
            "/layer.6/attention/self/Concat_3_output_0\n",
            "/layer.6/attention/self/Reshape_3_output_0\n",
            "/layer.6/attention/output/dense/MatMul_output_0\n",
            "/layer.6/attention/output/dense/Add_output_0\n",
            "/layer.6/attention/output/Add_output_0\n",
            "/layer.6/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.6/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.6/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.6/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.6/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.6/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.6/attention/output/LayerNorm/Add_output_0\n",
            "/layer.6/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.6/attention/output/LayerNorm/Div_output_0\n",
            "/layer.6/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.6/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.6/intermediate/dense/MatMul_output_0\n",
            "/layer.6/intermediate/dense/Add_output_0\n",
            "/layer.6/intermediate/Constant_output_0\n",
            "/layer.6/intermediate/Div_output_0\n",
            "/layer.6/intermediate/Erf_output_0\n",
            "/layer.6/intermediate/Constant_1_output_0\n",
            "/layer.6/intermediate/Add_output_0\n",
            "/layer.6/intermediate/Mul_output_0\n",
            "/layer.6/intermediate/Constant_2_output_0\n",
            "/layer.6/intermediate/Mul_1_output_0\n",
            "/layer.6/output/dense/MatMul_output_0\n",
            "/layer.6/output/dense/Add_output_0\n",
            "/layer.6/output/Add_output_0\n",
            "/layer.6/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.6/output/LayerNorm/Sub_output_0\n",
            "/layer.6/output/LayerNorm/Constant_output_0\n",
            "/layer.6/output/LayerNorm/Pow_output_0\n",
            "/layer.6/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.6/output/LayerNorm/Constant_1_output_0\n",
            "/layer.6/output/LayerNorm/Add_output_0\n",
            "/layer.6/output/LayerNorm/Sqrt_output_0\n",
            "/layer.6/output/LayerNorm/Div_output_0\n",
            "/layer.6/output/LayerNorm/Mul_output_0\n",
            "/layer.6/output/LayerNorm/Add_1_output_0\n",
            "/layer.7/attention/self/query/MatMul_output_0\n",
            "/layer.7/attention/self/query/Add_output_0\n",
            "/layer.7/attention/self/key/MatMul_output_0\n",
            "/layer.7/attention/self/key/Add_output_0\n",
            "/layer.7/attention/self/value/MatMul_output_0\n",
            "/layer.7/attention/self/value/Add_output_0\n",
            "/layer.7/attention/self/Shape_output_0\n",
            "/layer.7/attention/self/Constant_output_0\n",
            "/layer.7/attention/self/Gather_output_0\n",
            "/layer.7/attention/self/Shape_1_output_0\n",
            "/layer.7/attention/self/Constant_1_output_0\n",
            "/layer.7/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_1128\n",
            "/layer.7/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_1130\n",
            "/layer.7/attention/self/Unsqueeze_1_output_0\n",
            "/layer.7/attention/self/Constant_2_output_0\n",
            "/layer.7/attention/self/Constant_3_output_0\n",
            "/layer.7/attention/self/Concat_output_0\n",
            "/layer.7/attention/self/Reshape_output_0\n",
            "/layer.7/attention/self/Transpose_output_0\n",
            "/layer.7/attention/self/Shape_2_output_0\n",
            "/layer.7/attention/self/Constant_4_output_0\n",
            "/layer.7/attention/self/Gather_2_output_0\n",
            "/layer.7/attention/self/Shape_3_output_0\n",
            "/layer.7/attention/self/Constant_5_output_0\n",
            "/layer.7/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_1145\n",
            "/layer.7/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_1147\n",
            "/layer.7/attention/self/Unsqueeze_3_output_0\n",
            "/layer.7/attention/self/Constant_6_output_0\n",
            "/layer.7/attention/self/Constant_7_output_0\n",
            "/layer.7/attention/self/Concat_1_output_0\n",
            "/layer.7/attention/self/Reshape_1_output_0\n",
            "/layer.7/attention/self/Shape_4_output_0\n",
            "/layer.7/attention/self/Constant_8_output_0\n",
            "/layer.7/attention/self/Gather_4_output_0\n",
            "/layer.7/attention/self/Shape_5_output_0\n",
            "/layer.7/attention/self/Constant_9_output_0\n",
            "/layer.7/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_1161\n",
            "/layer.7/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_1163\n",
            "/layer.7/attention/self/Unsqueeze_5_output_0\n",
            "/layer.7/attention/self/Constant_10_output_0\n",
            "/layer.7/attention/self/Constant_11_output_0\n",
            "/layer.7/attention/self/Concat_2_output_0\n",
            "/layer.7/attention/self/Reshape_2_output_0\n",
            "/layer.7/attention/self/Transpose_1_output_0\n",
            "/layer.7/attention/self/Transpose_2_output_0\n",
            "/layer.7/attention/self/MatMul_output_0\n",
            "/layer.7/attention/self/Constant_12_output_0\n",
            "/layer.7/attention/self/Div_output_0\n",
            "/layer.7/attention/self/Add_output_0\n",
            "/layer.7/attention/self/Softmax_output_0\n",
            "/layer.7/attention/self/MatMul_1_output_0\n",
            "/layer.7/attention/self/Transpose_3_output_0\n",
            "/layer.7/attention/self/Shape_6_output_0\n",
            "/layer.7/attention/self/Constant_13_output_0\n",
            "/layer.7/attention/self/Gather_6_output_0\n",
            "/layer.7/attention/self/Shape_7_output_0\n",
            "/layer.7/attention/self/Constant_14_output_0\n",
            "/layer.7/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_1186\n",
            "/layer.7/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_1188\n",
            "/layer.7/attention/self/Unsqueeze_7_output_0\n",
            "/layer.7/attention/self/Constant_15_output_0\n",
            "/layer.7/attention/self/Concat_3_output_0\n",
            "/layer.7/attention/self/Reshape_3_output_0\n",
            "/layer.7/attention/output/dense/MatMul_output_0\n",
            "/layer.7/attention/output/dense/Add_output_0\n",
            "/layer.7/attention/output/Add_output_0\n",
            "/layer.7/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.7/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.7/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.7/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.7/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.7/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.7/attention/output/LayerNorm/Add_output_0\n",
            "/layer.7/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.7/attention/output/LayerNorm/Div_output_0\n",
            "/layer.7/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.7/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.7/intermediate/dense/MatMul_output_0\n",
            "/layer.7/intermediate/dense/Add_output_0\n",
            "/layer.7/intermediate/Constant_output_0\n",
            "/layer.7/intermediate/Div_output_0\n",
            "/layer.7/intermediate/Erf_output_0\n",
            "/layer.7/intermediate/Constant_1_output_0\n",
            "/layer.7/intermediate/Add_output_0\n",
            "/layer.7/intermediate/Mul_output_0\n",
            "/layer.7/intermediate/Constant_2_output_0\n",
            "/layer.7/intermediate/Mul_1_output_0\n",
            "/layer.7/output/dense/MatMul_output_0\n",
            "/layer.7/output/dense/Add_output_0\n",
            "/layer.7/output/Add_output_0\n",
            "/layer.7/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.7/output/LayerNorm/Sub_output_0\n",
            "/layer.7/output/LayerNorm/Constant_output_0\n",
            "/layer.7/output/LayerNorm/Pow_output_0\n",
            "/layer.7/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.7/output/LayerNorm/Constant_1_output_0\n",
            "/layer.7/output/LayerNorm/Add_output_0\n",
            "/layer.7/output/LayerNorm/Sqrt_output_0\n",
            "/layer.7/output/LayerNorm/Div_output_0\n",
            "/layer.7/output/LayerNorm/Mul_output_0\n",
            "/layer.7/output/LayerNorm/Add_1_output_0\n",
            "/layer.8/attention/self/query/MatMul_output_0\n",
            "/layer.8/attention/self/query/Add_output_0\n",
            "/layer.8/attention/self/key/MatMul_output_0\n",
            "/layer.8/attention/self/key/Add_output_0\n",
            "/layer.8/attention/self/value/MatMul_output_0\n",
            "/layer.8/attention/self/value/Add_output_0\n",
            "/layer.8/attention/self/Shape_output_0\n",
            "/layer.8/attention/self/Constant_output_0\n",
            "/layer.8/attention/self/Gather_output_0\n",
            "/layer.8/attention/self/Shape_1_output_0\n",
            "/layer.8/attention/self/Constant_1_output_0\n",
            "/layer.8/attention/self/Gather_1_output_0\n",
            "onnx::Unsqueeze_1250\n",
            "/layer.8/attention/self/Unsqueeze_output_0\n",
            "onnx::Unsqueeze_1252\n",
            "/layer.8/attention/self/Unsqueeze_1_output_0\n",
            "/layer.8/attention/self/Constant_2_output_0\n",
            "/layer.8/attention/self/Constant_3_output_0\n",
            "/layer.8/attention/self/Concat_output_0\n",
            "/layer.8/attention/self/Reshape_output_0\n",
            "/layer.8/attention/self/Transpose_output_0\n",
            "/layer.8/attention/self/Shape_2_output_0\n",
            "/layer.8/attention/self/Constant_4_output_0\n",
            "/layer.8/attention/self/Gather_2_output_0\n",
            "/layer.8/attention/self/Shape_3_output_0\n",
            "/layer.8/attention/self/Constant_5_output_0\n",
            "/layer.8/attention/self/Gather_3_output_0\n",
            "onnx::Unsqueeze_1267\n",
            "/layer.8/attention/self/Unsqueeze_2_output_0\n",
            "onnx::Unsqueeze_1269\n",
            "/layer.8/attention/self/Unsqueeze_3_output_0\n",
            "/layer.8/attention/self/Constant_6_output_0\n",
            "/layer.8/attention/self/Constant_7_output_0\n",
            "/layer.8/attention/self/Concat_1_output_0\n",
            "/layer.8/attention/self/Reshape_1_output_0\n",
            "/layer.8/attention/self/Shape_4_output_0\n",
            "/layer.8/attention/self/Constant_8_output_0\n",
            "/layer.8/attention/self/Gather_4_output_0\n",
            "/layer.8/attention/self/Shape_5_output_0\n",
            "/layer.8/attention/self/Constant_9_output_0\n",
            "/layer.8/attention/self/Gather_5_output_0\n",
            "onnx::Unsqueeze_1283\n",
            "/layer.8/attention/self/Unsqueeze_4_output_0\n",
            "onnx::Unsqueeze_1285\n",
            "/layer.8/attention/self/Unsqueeze_5_output_0\n",
            "/layer.8/attention/self/Constant_10_output_0\n",
            "/layer.8/attention/self/Constant_11_output_0\n",
            "/layer.8/attention/self/Concat_2_output_0\n",
            "/layer.8/attention/self/Reshape_2_output_0\n",
            "/layer.8/attention/self/Transpose_1_output_0\n",
            "/layer.8/attention/self/Transpose_2_output_0\n",
            "/layer.8/attention/self/MatMul_output_0\n",
            "/layer.8/attention/self/Constant_12_output_0\n",
            "/layer.8/attention/self/Div_output_0\n",
            "/layer.8/attention/self/Add_output_0\n",
            "/layer.8/attention/self/Softmax_output_0\n",
            "/layer.8/attention/self/MatMul_1_output_0\n",
            "/layer.8/attention/self/Transpose_3_output_0\n",
            "/layer.8/attention/self/Shape_6_output_0\n",
            "/layer.8/attention/self/Constant_13_output_0\n",
            "/layer.8/attention/self/Gather_6_output_0\n",
            "/layer.8/attention/self/Shape_7_output_0\n",
            "/layer.8/attention/self/Constant_14_output_0\n",
            "/layer.8/attention/self/Gather_7_output_0\n",
            "onnx::Unsqueeze_1308\n",
            "/layer.8/attention/self/Unsqueeze_6_output_0\n",
            "onnx::Unsqueeze_1310\n",
            "/layer.8/attention/self/Unsqueeze_7_output_0\n",
            "/layer.8/attention/self/Constant_15_output_0\n",
            "/layer.8/attention/self/Concat_3_output_0\n",
            "/layer.8/attention/self/Reshape_3_output_0\n",
            "/layer.8/attention/output/dense/MatMul_output_0\n",
            "/layer.8/attention/output/dense/Add_output_0\n",
            "/layer.8/attention/output/Add_output_0\n",
            "/layer.8/attention/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.8/attention/output/LayerNorm/Sub_output_0\n",
            "/layer.8/attention/output/LayerNorm/Constant_output_0\n",
            "/layer.8/attention/output/LayerNorm/Pow_output_0\n",
            "/layer.8/attention/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.8/attention/output/LayerNorm/Constant_1_output_0\n",
            "/layer.8/attention/output/LayerNorm/Add_output_0\n",
            "/layer.8/attention/output/LayerNorm/Sqrt_output_0\n",
            "/layer.8/attention/output/LayerNorm/Div_output_0\n",
            "/layer.8/attention/output/LayerNorm/Mul_output_0\n",
            "/layer.8/attention/output/LayerNorm/Add_1_output_0\n",
            "/layer.8/intermediate/dense/MatMul_output_0\n",
            "/layer.8/intermediate/dense/Add_output_0\n",
            "/layer.8/intermediate/Constant_output_0\n",
            "/layer.8/intermediate/Div_output_0\n",
            "/layer.8/intermediate/Erf_output_0\n",
            "/layer.8/intermediate/Constant_1_output_0\n",
            "/layer.8/intermediate/Add_output_0\n",
            "/layer.8/intermediate/Mul_output_0\n",
            "/layer.8/intermediate/Constant_2_output_0\n",
            "/layer.8/intermediate/Mul_1_output_0\n",
            "/layer.8/output/dense/MatMul_output_0\n",
            "/layer.8/output/dense/Add_output_0\n",
            "/layer.8/output/Add_output_0\n",
            "/layer.8/output/LayerNorm/ReduceMean_output_0\n",
            "/layer.8/output/LayerNorm/Sub_output_0\n",
            "/layer.8/output/LayerNorm/Constant_output_0\n",
            "/layer.8/output/LayerNorm/Pow_output_0\n",
            "/layer.8/output/LayerNorm/ReduceMean_1_output_0\n",
            "/layer.8/output/LayerNorm/Constant_1_output_0\n",
            "/layer.8/output/LayerNorm/Add_output_0\n",
            "/layer.8/output/LayerNorm/Sqrt_output_0\n",
            "/layer.8/output/LayerNorm/Div_output_0\n",
            "/layer.8/output/LayerNorm/Mul_output_0\n",
            "output\n",
            "596M\tbert-base-multilingual-cased-maxlayer9.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "COPY_TO_DRIVE=True\n",
        "if COPY_TO_DRIVE:\n",
        "  from google.colab import drive\n",
        "  driveroot = '/content/gdrive'\n",
        "  drive.mount(driveroot, force_remount=True)\n",
        "  drivedir=f'{driveroot}/MyDrive'\n",
        "  subdir=f'{drivedir}/awesome'\n",
        "  !mkdir -p $subdir\n",
        "  onnxname=onnxpath.split('/')[-1]\n",
        "  onnxto=f'{subdir}/{onnxname}'\n",
        "  print(onnxto)\n",
        "  print(onnxpath)\n",
        "  !cp $onnxpath $onnxto\n",
        "  !du -h $onnxto\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfuHcEmjWUWF",
        "outputId": "377f4240-413d-4ce6-a647-4fa89fbd89ac"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/awesome/bert-base-multilingual-cased-maxlayer8.onnx\n",
            "bert-base-multilingual-cased-maxlayer8.onnx\n",
            "569M\t/content/gdrive/MyDrive/awesome/bert-base-multilingual-cased-maxlayer8.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $driveroot\n"
      ],
      "metadata": {
        "id": "Gjprlr3Fj22K",
        "outputId": "1bfac035-8c13-4e32-9ed3-e742183c7faf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive  Othercomputers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-processing\n",
        "def wstok(x): return x.strip().split()\n",
        "def subwords(xs): return [tokenizer.tokenize(x) for x in xs]\n",
        "def ids(xs): return [tokenizer.convert_tokens_to_ids(x) for x in xs]\n",
        "sent_src, sent_tgt = wstok(src), wstok(tgt)\n",
        "token_src, token_tgt = subwords(sent_src), subwords(sent_tgt)\n",
        "wid_src, wid_tgt = ids(token_src), ids(token_tgt)\n",
        "#def tokenizer_max_len(tokenizer): return tokenizer.max_len_single_sentence if hasattr(tokenizer, 'max_len_single_sentence') else tokenizer.model_max_length\n",
        "maxlenkw = {}\n",
        "if hasattr(tokenizer, 'model_max_length'):\n",
        "  maxlenkw['model_max_length'] = tokenizer.model_max_length\n",
        "  maxlenkw['truncation'] = True\n",
        "else:\n",
        "  maxlenkw['max_length'] = tokenizer.max_len\n",
        "\n",
        "def ids_for_model(ids, model, tokenizer): return tokenizer.prepare_for_model(list(itertools.chain(*ids)), return_tensors='pt', **maxlenkw)['input_ids']\n",
        "print(f'wid {len(wid_src)} x {len(wid_tgt)}')\n",
        "ids_src, ids_tgt = ids_for_model(wid_src, model, tokenizer), ids_for_model(wid_tgt, model, tokenizer)\n",
        "print(f'{ids_src}')\n",
        "print(f'{ids_tgt}')\n",
        "sub2word_map_src = []\n",
        "for i, word_list in enumerate(token_src):\n",
        "  sub2word_map_src += [i for x in word_list]\n",
        "sub2word_map_tgt = []\n",
        "for i, word_list in enumerate(token_tgt):\n",
        "  sub2word_map_tgt += [i for x in word_list]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YabaixKkldP_",
        "outputId": "36c1a3ea-f000-4474-b3b5-98a30f4fd7ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wid 14 x 15\n",
            "tensor([[  101,   146, 28870,   169, 10751, 13000, 12373,   146, 10134, 19090,\n",
            "         11222,   169, 15607, 57156, 22859,   119,   102]])\n",
            "tensor([[  101, 16680, 52302, 10333, 10119, 18257, 15249, 16348, 14645, 46481,\n",
            "         10183, 10153, 22859, 10104, 10109, 16689, 22757,   119,   102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUa-ZqUxZ8Z"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l $onnxpath\n",
        "import onnxruntime as ort\n",
        "import onnxruntime as rt\n",
        "\n",
        "align_layer_max = 9\n",
        "onnxpath = onnxpathm(align_layer_max)\n",
        "\n",
        "def onnx_inputs(path, inputs=None):\n",
        "  if True or inputs is None:\n",
        "    return [x.name for x in onnx.load(path).graph.input]\n",
        "  return inputs\n",
        "\n",
        "import onnx\n",
        "\n",
        "def modify_onnx_outputs(path, onnxpathout, outputs, inputs=None, checker=True):\n",
        "  if inputs is not None:\n",
        "    onnx.utils.extract_model(path, onnxpathout, onnx_inputs(path, inputs), outputs)\n",
        "  else:\n",
        "    model = onnx.load(path)\n",
        "    outset = set(x.name for x in model.graph.output)\n",
        "    for x in outputs:\n",
        "      if x not in outset:\n",
        "        print(f'WARN: missing output {x} in {list(onnx_helper.enumerate_model_node_outputs(model))[-99:]}')\n",
        "        return path\n",
        "    model = onnx_helper.select_model_inputs_outputs(model, outputs=outputs, inputs=None)\n",
        "    onnx.save(model, onnxpathout)\n",
        "  if checker:\n",
        "    onnx.checker.check_model(onnxpathout)\n",
        "  return onnxpathout\n",
        "\n",
        "\n",
        "def onnxmask(ids):\n",
        "  #return (ids != 0).to(torch.float32)\n",
        "  return make_extended_mask(ids)[0, 0, :, :]\n",
        "\n",
        "def onnx_word_encs(session, ids_src):\n",
        "  msrc = onnxmask(ids_src)\n",
        "  osrc = session.run(output_names, {input_names[0]: ids_src.numpy(), input_names[1]: msrc.numpy()})[0][:,1:-1,:]\n",
        "  return torch.tensor(osrc)\n",
        "\n",
        "def alignpairs(out_src, out_tgt, sub2word_map_src, sub2word_map_tgt, threshold):\n",
        "      dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
        "\n",
        "      softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
        "      softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
        "\n",
        "      # tryalso entmax15(dot_prod, dim=...)? also TODO: before softmax mask off cls sep pad tokens\n",
        "\n",
        "      softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
        "\n",
        "      align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
        "      align_words = set()\n",
        "      for xyz in align_subwords:\n",
        "        i, j = xyz[-2], xyz[-1]\n",
        "        #print(f'subword: {i}-{j}')\n",
        "        align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
        "      return sorted(list(align_words))\n",
        "\n",
        "sess_options = rt.SessionOptions()\n",
        "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "\n",
        "for use_output in ['output', '/layer.7/output/LayerNorm/Add_1_output_0']:\n",
        "  print(use_output)\n",
        "\n",
        "  session_path = onnxpath\n",
        "  if use_output is not None and use_output != 'output':\n",
        "    onnxpathout = f'{onnxpath}.out.{use_output.replace(\"/\",\"_\")}'\n",
        "    assert onnxpathout != onnxpath\n",
        "    print(onnxpathout)\n",
        "    session_path = modify_onnx_outputs(onnxpath, onnxpathout, outputs=[use_output], inputs=['input_ids', 'attention_mask'])\n",
        "\n",
        "  !du -h $session_path\n",
        "  session = ort.InferenceSession(session_path, sess_options=sess_options, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "\n",
        "  input_names = [x.name for x in session.get_inputs()]\n",
        "  output_names = [x.name for x in session.get_outputs()]\n",
        "  print(input_names)\n",
        "  print(output_names)\n",
        "\n",
        "  osrc = onnx_word_encs(session, ids_src)\n",
        "  otgt = onnx_word_encs(session, ids_tgt)\n",
        "  del session\n",
        "  print(f'{osrc.size()} x {otgt.size()}')\n",
        "\n",
        "  for threshold in [1e-3, 1e-8]:\n",
        "    align_words = alignpairs(osrc, otgt, sub2word_map_src, sub2word_map_tgt, threshold)\n",
        "    # 13-14 2-1 10-8 0-0 5-4 4-2 12-9 11-12 3-3 8-6 1-0 7-5 9-7\n",
        "    # [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (12, 9), (13, 14)]\n",
        "    print_align(align_words, f'onnx {use_output} {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3FlRpS-dYFv",
        "outputId": "bd0652f8-6607-402a-82fa-3c0f64bdd2a9"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 624160622 Apr  5 21:18 bert-base-multilingual-cased-maxlayer9.onnx\n",
            "output\n",
            "596M\tbert-base-multilingual-cased-maxlayer9.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['input_ids', 'attention_mask']\n",
            "['output']\n",
            "torch.Size([1, 15, 768]) x torch.Size([1, 17, 768])\n",
            "onnx output 0.001 12 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            "onnx output 1e-08 13 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            "/layer.7/output/LayerNorm/Add_1_output_0\n",
            "bert-base-multilingual-cased-maxlayer9.onnx.out._layer.7_output_LayerNorm_Add_1_output_0\n",
            "569M\tbert-base-multilingual-cased-maxlayer9.onnx.out._layer.7_output_LayerNorm_Add_1_output_0\n",
            "['input_ids', 'attention_mask']\n",
            "['/layer.7/output/LayerNorm/Add_1_output_0']\n",
            "torch.Size([1, 15, 768]) x torch.Size([1, 17, 768])\n",
            "onnx /layer.7/output/LayerNorm/Add_1_output_0 0.001 13 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            "onnx /layer.7/output/LayerNorm/Add_1_output_0 1e-08 14 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 10), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mde\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# alignment\n",
        "\n",
        "def sents_without_startend(batch): return batch[:, 1:-1]\n",
        "if USE_AWESOME_ALIGN:\n",
        "  def hiddens(model, ids, align_layer):\n",
        "    return sents_without_startend(model.bert(ids, align_layer=align_layer, attention_mask=(ids!=0)))\n",
        "else:\n",
        "  def hidden(model, ids): return model(ids.unsqueeze(0), output_hidden_states=True)[2]\n",
        "  def hiddens(model, ids, align_layer):\n",
        "    return sents_without_startend(hidden(model, ids)[align_layer])\n",
        "\n",
        "\n",
        "DECODE_AWESOME_ALIGN=False\n",
        "\n",
        "for align_layer in range(max(0,align_layer_max - 2),align_layer_max+1):\n",
        " last_align = None\n",
        " threshold = 1e-3\n",
        " for it in range(6):\n",
        "  if DECODE_AWESOME_ALIGN and USE_AWESOME_ALIGN:\n",
        "    # get_aligned_word takes a batch.\n",
        "    # print(f'{len(ids_src[0])} x {len(ids_tgt[0])}')\n",
        "    align_words = model.get_aligned_word(ids_src, ids_tgt, (sub2word_map_src,), (sub2word_map_tgt,), 'cpu', len(ids_src), len(ids_tgt), align_layer, 'softmax', threshold, True)[0]\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      out_src = hiddens(model, ids_src, align_layer)\n",
        "      out_tgt = hiddens(model, ids_tgt, align_layer)\n",
        "      #pdb.set_trace()\n",
        "      #out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "      #out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "      align_words = alignpairs(out_src, out_tgt, sub2word_map_src, sub2word_map_tgt, threshold)\n",
        "\n",
        "  align_words = sorted(list(align_words))\n",
        "  if align_words != last_align:\n",
        "    print_align(align_words,desc = f' (layer {align_layer} > {threshold:.3g})')\n",
        "\n",
        "  last_align = align_words\n",
        "  threshold = threshold * 1e-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn-h-WHTl2fm",
        "outputId": "f6bac563-6bf3-49f2-b75b-c4ba6f028ddd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (layer 7 > 0.001) 10 links [(2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 7 > 0.0001) 12 links [(1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 7 > 1e-05) 11 links [(1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 7 > 1e-06) 12 links [(2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 11), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mla\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 7 > 1e-07) 11 links [(1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 7 > 1e-08) 12 links [(1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 8 > 0.001) 13 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 8 > 1e-05) 13 links [(1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 11), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mla\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 8 > 1e-06) 13 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 9 > 0.001) 13 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 9 > 0.0001) 12 links [(1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 13), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91medad\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 9 > 1e-05) 13 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 9 > 1e-06) 14 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (11, 13), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91medad\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 9 > 1e-07) 13 links [(0, 0), (1, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94mbought\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n",
            " (layer 9 > 1e-08) 13 links [(0, 0), (2, 1), (3, 3), (4, 2), (5, 4), (7, 5), (8, 6), (9, 7), (10, 8), (11, 12), (11, 13), (12, 9), (13, 14)] for 'I bought a new car because I was going through a midlife crisis .' to 'Compré un auto nuevo porque estaba pasando por una crisis de la mediana edad .'\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mCompré\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91mun\u001b[0m\n",
            "\u001b[1m\u001b[94mnew\u001b[0m===\u001b[1m\u001b[91mnuevo\u001b[0m\n",
            "\u001b[1m\u001b[94mcar\u001b[0m===\u001b[1m\u001b[91mauto\u001b[0m\n",
            "\u001b[1m\u001b[94mbecause\u001b[0m===\u001b[1m\u001b[91mporque\u001b[0m\n",
            "\u001b[1m\u001b[94mwas\u001b[0m===\u001b[1m\u001b[91mestaba\u001b[0m\n",
            "\u001b[1m\u001b[94mgoing\u001b[0m===\u001b[1m\u001b[91mpasando\u001b[0m\n",
            "\u001b[1m\u001b[94mthrough\u001b[0m===\u001b[1m\u001b[91mpor\u001b[0m\n",
            "\u001b[1m\u001b[94ma\u001b[0m===\u001b[1m\u001b[91muna\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91mmediana\u001b[0m\n",
            "\u001b[1m\u001b[94mmidlife\u001b[0m===\u001b[1m\u001b[91medad\u001b[0m\n",
            "\u001b[1m\u001b[94mcrisis\u001b[0m===\u001b[1m\u001b[91mcrisis\u001b[0m\n",
            "\u001b[1m\u001b[94m.\u001b[0m===\u001b[1m\u001b[91m.\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}